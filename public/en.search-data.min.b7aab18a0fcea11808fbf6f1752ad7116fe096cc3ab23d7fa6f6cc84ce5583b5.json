[{"id":0,"href":"/docs/01-introduccion/","title":"Introducción","section":"Docs","content":" Introducción # the #1 rule of distribute computing: Don’t distribute your computing! At least if you can in any way avoid it\nYou\u0026rsquo;re not Google. Your company will never be Google\u0026hellip; Is there a reason we can\u0026rsquo;t just do this all in Postgres?\nAvances imporantes han ocurrido en las últimas décadas:\nDesarrollo de microprocesadores potentes, pequeños y ecónomicos. Avance de las tecnologías de comunicaciones. Miniaturización de los sistemas de cómputo (ES, IoT, SoCs, etc). En la actualidad es relativamente sencillo desarrollar un sistema compuesto de múltiples computadoras conectadas por una red.\nAl estar las computadoras físicamente separadas se habla de un sistema distribuido.\nDefinición # You know you have a distributed system when the crash of a computer you have never heard of stops you from getting any work done. \u0026ndash; Leslie Lamport\nUna colección de elementos computacionales autónomos que dan la apariencia a sus usuarios de ser un sistema coherente \u0026ndash; Tanenbaum y Van Steen\nAunque no existe una definición ampliamente aceptada por toda la disciplina, la distribución de los componentes en diferentes sistemas comunicados mediante una red es una característica común.\nCaracteristica 1: elementos independientes # Nodos independientes que colaboran para alcanzar un objetivo común. Los nodos son heterogéneos. La comunicación entre los nodos se realiza mediante paso de mensajes. No se existe un reloj global (dificulta la sincronización y coordinación). La concurrencia y el paralelismo es natural. Organizado como una red superpuesta, estructurada o no-estructurada (ej: sistemas p2p). Fallas parciales (independientes). Se tiene que resolver cuestiones de organización y membresía (grupos cerrados, abiertos). Caracteristica 2: sistema coherente # El sistema se comporta de acuerdo a las expectativas de sus usuarios. Transparencia de distribución: no importa como, cuando ni donde se conecte al sistema, el usuario debe tener el mismos servicio. Sin embargo, no es posible (ni deseable) ocultar todos los detalles de la distribución del sistema. Fundamental poder lidiar con fallas parciales. Distribuido vs Descentralizado # Conceptos relacionados\nSistema distribuido: componentes colaboran para realizar una tarea o proveer un servicio.\nSistema descentralizado: componentes con mayor autonomía sin único punto de control.\nUn sistema distribuido puede estar logicamente centralizado: DNS.\nUn sistema descentralizado no tiene una autoridad central: blockchain (ej. Bitcoin)\nOtra vision es la siguiente:\nSistema integrativo (conectar sistemas existentes formando así uno nuevo) Sistema expansivo (agregar nodos a un sistema existente). Luego:\nSistema descentralizado: visión integrativa, los recursos se encuentran necesariamente dispersos. Sistema distribuido: visión expansiva, los recursos se encuentran suficientemente dispersos. Complejidad # Los sistemas distribuidos son inherentemente complejos. Los sistemas centralizados son más sencillos. La distribución no es un fin en sí mismo: considerar soluciones lo más simples posibles. Middleware # Los componentes y funciones comunmente usados en un sistema distribuido se agrupan en un middleware, una capa de software entre el sistema operativo y las aplicaciones que intenta abstraer los detalles escabrosos y ofrecer una interfaz más amigable.\nPor ejemplo, un middleware ofrece:\nComunicación (RPC, RMI, paso de mensajes, etc.) Manejo de transacciones. Composición de servicios. Confiabilidad. Objetivos de diseño # Sólo por que sea posible no quiere decir que diseñar un sistema distribuido sea siempre una buena idea.\nUn sistema distribuido debe poder satisfacer alguno de los siguientes objetivos, para que su implementación valga la pena:\nPermitir que los recursos sean más fácilmente accesibles. Ocultar en lo posible que los recursos están desperdigados (transparencia de distribución). Debe ser abierto. Debe poder ser escalable. Compartir recursos # Por cuestiones económicas. Mejorar la colaboración. Ejemplo clásico: p2p Transparencia de distribución # Un sistema distribuido debe (en lo posible) ocultar que los procesos y recursos estan fisicamente distribuidos: esto se conoce como transparencia de distribución.\nDiversos tipos:\nAcceso: ocultar cómo se accede al recurso y la representación de datos. Ubicación: ocultar la ubicación de un recurso. Reubicación: ocultar el hecho de que el recurso pueda cambiar su ubicación mientras esta en uso. Migración: ocultar el hecho de que un recurso cambie su ubicación. Replicación: ocultar el hecho de que existan múltiples copias de un mismo recurso. Concurrencia: ocultar el hecho de que un recurso pueda ser accedido por múltiples usuarios. Falla: ocultar la falla y recuperación de un recurso u objeto. No siempre es deseable o posible alcanzar el máximo grado de transparencia.\nEs imposible ocultar las latencias de una red WAN u ocultar la falla de un nodo. Compromiso entre el nivel de transparencia y perfomance: Mantener las replicas consistentes incurre en un costo temporal que no se puede ocultar. Se puede argumentar que es mejor exponer la distribución al usuario, en lugar de ocultarla. Abierto # Poder interactuar con otros sistemas. Requiere interfaces bien definidas.\nUn sistema abierto es aquel que permite que sus componentes sean utilizados en otros sistemas. También generalmente un sistema abierto esta compuesto por componentes de otros sistemas. Beneficia la interoperabilidad, portabilidad, composibility y extensibilidad. Una característica importante para lograr este objetivo es la de separar política de mecanismo, evitando soluciones monolíticas.\nEscalabilidad # La escalabilidad abarca tres dimensiones:\nTamaño: la facilidad con que se pueden sumar usuarios o recursos.\nGeneralmente relacionado con limites en capacidad de cómputo, almacenamiento y ancho de banda. Pueden ser formalmente analizados mediante teoría de colas. Geografía: los recursos y usuarios pueden estar desperdigados pero las latencias no afectan seriamente al sistema.\nEl principal problema es la comunicación sincrónica sobre enlaces con alta latencia. Las WANs ofrecen menor confiabilidad que una LAN, menor capacidad de ancho de banda. ¿Multicast / Broadcast? Posible en LANs, no tan así en WANs. Administrativa: el sistema abarca distintas unidades organizacionales pero aún así es fácilmente administrable.\nCómo resolver conflictos de políticas acerca de uso, pago, administración, seguridad, etc. Ejemplos:\nTamaño: incrementar fácilmente el número de usuarios o procesos. Geografía: poder aumentar la distancia entre nodos. Administrativo: integrar recursos de otra organización. Soluciones:\nEscalar verticalmente: simplemente incrementar la capacidad del servicio (computo, almacenamiento o ancho de banda). Escalar horizontalmente: Ocultar latencias: comunicación asíncrona, fat-clients, etc. Distribuir el trabajo: dividir un componente y dispersarlo por el sistema. Ej: mover computo al cliente (Java Applet), descentralizar un servicio (DNS), descentralizar contenido (WWW), etc. Replicar: contar con una copia cercana, cache, etc. Problemas de consistencia. Consistencia estricta requiere sincronización global (costoso, reduce escalabilidad). Tipos de sistemas distribuidos # A grandes rasgos, podemos clasificar los sistemas distribuidos en sistemas distribuidos de cómputo, de información y pervasivos.\nCómputo: Cluster: conjunto de sistemas interconectados por una red de alta velocidad. Grid: nodos dispersos, heterogéneos, diferentes organizaciones. Cloud: software/infraestructura como servicio. Edge Información: Integración de sistemas de información ya existentes; ofrecer servicios como transacciones distribuidas. Pervasiva: Sistemas móviles, embebidos, IoT. No existe una topología estática, ni conexión permanente, etc. Tres tipos: ubicuos, móviles y redes de sensores (los límites entre la categorías son difusos). Falacias # Desarrollar un (buen) sistema distribuido es una tarea ardua. Las siguientes falsas presunciones durante el diseño del sistema, traen como consecuencia complejidad innecesaria y errores:\nLa red es confiable. La red es segura. La red es homogenea. La topología no cambia. La latencia es cero. El ancho de banda es infinito. El costo del transporte es cero. Sólo existe un administrador. Algunas más:\nUn sistema centralizado no escala: Un sistema físicamente centralizado quizá no, pero uno lógicamente centralizado sí (ejemplo DNS). Un sistema centralizado tiene un unico punto de falla: Si, pero un solo punto de falla es más fácil de administrar, más fácil de arreglar. Soluciones: en el caso de DNS, cada root server es a su vez un cluster. ¿Que vamos a estudiar? # Arquitectura: ¿Como organizar el sistema? Procesos: ¿Procesos, hilos? Comunicacion: ¿Como comunicar entre los nodos? Coordinación: ¿Cómo coordinar acciones? ¿Y cómo hacerlo de una manera independiente de la aplicación? Nombres: ¿Cómo identificar los diferentes recursos? Consistencia y replicación: Si se replica, ¿como se maneja la consistencia? Tolerancia a fallas: como mantener el sistema funcionando ante la falla de un componente. Seguridad: asegurar acceso autorizado a los recursos. "},{"id":1,"href":"/docs/02-arquitectura/","title":"Arquitecturas","section":"Docs","content":" Arquitecturas # Es fundamental una correcta organización para administrar la complejidad de un sistema distribuido.\nPodemos diferenciar:\nLa organización de los componentes de software (arquitectura de software). Cómo están físicamente instanciados (arquitectura del sistema). Estilo arquitectonico # Organización lógica de los componentes de software del sistema:\nComponentes. Unidad modular con interfaces bien definidas (reemplazable). Cómo se conectan y comunican. Conector: el mecanismo que media la comunicación entre los componentes. Qué datos intercambian. Cómo están configurados. Según como se configuran componentes y conectores, tenemos una arquitectura de software.\nArquitecturas tipicamente utilizadas en sistemas distribuidos son:\nArquitecturas por capas. Orientadas a los servicios. Publish-subscribe. Arquitectura por capas # Los componentes se organizan en capas. Un componente en la capa N invoca generalmente los servicios de un componente en la capa N-1. Excepcionalmente, un componente puede invocar un servicio de una capa superior (N+1).\nEjemplo clásico: protocolos de comunicación de redes (TCP/IP, OSI, etc).\nMuchas aplicaciones se organizan en capas siguiendo el siguiente estilo:\nCapa de presentación o interfaz de usuario. Capa de procesamiento o de negocio. Capa de datos o persistencia. Desventaja:\nExiste una dependencia fuerte entre las distintas capas. Orientadas a los servicios # Organización más imprecisa, donde cada componente encapsula un servicio. El sistema se estructura como una composición de servicios.\nOrientado a objetos (invocación de objetos remotos).\nMicroservicios (cada componente representa un servicio separado e independiente).\nBasada en recursos (REST).\nObjetos:\nCada componente corresponde con objeto. Se comunican mediante algún mecanismo de invocación. Encapsular datos y procedimientos dentro del objeto, ofreciendo una interface. Los objetos pueden estar distribuidos. Microservicios:\nMas info: https://microservices.io/ \u0026ldquo;same crap, but distributed\u0026rdquo; the biggest issue with microservices is that they convert nice errors with a stack trace to network errors SOA:\nLa aplicación es una composición de servicios. Estos pueden pertenecer a diferentes organizaciones administrativas. Ej: procesador de pagos. Basados en recursos:\nEn lugar de servicios, se consideran recursos. Popular por su simplicidad. Publish-subscribe # Separación de procesamiento y coordinación (comunicación y cooperación).\nEl sistema es visto como un conjunto de componentes autónomos.\nLograr que los componentes no tengan dependencias explicitas.\nLos componentes describen los eventos que le son de interés.\nEvent-based\nReferencialmente desacoplados Temporalmente acoplados Space-based\nReferencial y temporalmente desacoplados. Comunicación mediante tuplas Procesos ingresan tuplas en un espacio compartido. Recuperación mediante búsqueda Ambos tipos se pueden combinar (generar evento cuando tuplas de interés son ingresadas al espacio de intercambio)\nEn este caso, hablamos de publicación - subscripción.\n- Se deben describir los eventos de interes. - Generalmente como (atributo, valor) o (atributo, rango) El middleware # Facilita el desarrollo de un sistema distribuido. Es una capa que administra recursos y ofrece servicios comunes.\nEl principal objetivo es ayudar en la transparencia de distribución.\nPor ejemplo, se puede encargar de:\nAcceso a recursos remotos. Ofrecer servicios para la comunicación entre componentes. Servicios de seguridad y administración. Recuperación de fallas. Coordinación. Etc. Algunos ejemplos concretos en sistemas distribuidos:\nServicio de Mensajería: RabbitMQ, ZeroMQ, Llamadas a procedimientos remotos: RPC, RMI, gRPC Objetos distribuidos: CORBA Streaming de datos y eventos: Apache Kafka Monitores de transacciones distribuidas El middleware se puede organizar de varias maneras. Por ejemplo:\nWrappers: por aplicación o centralizado. Interceptores: Permite ejecutar código adicional durante la ejecución de un servicio. Arquitectura del sistema # Ubicación e interaccion de los componentes software junto con el hardware.\nPensar en clientes que piden servicios a servidores ayuda con la complejidad de los sistemas distribuidos.\nArquitectura en capas # Cliente-servidor:\nModelo más sencillo Procesos divididos en dos grupos: clientes que invocan un servicio implementado en servidores La invocacion puede usar una conexión no confiable (quiza usando operaciones idempotentes) o confiable (TCP, cuando la red no es fiable). No siempre se puede definir de manera precisa la separación entre cliente y servidor. Puede variar. Existen múltiples alternativas de como distribuir tres capas lógicas en un modelo cliente-servidor.\nDos capas (two-tier) Multicapa: distribuir las capas en múltiples máquinas, por ejemplo una arquitectura de 3-capas. Distribución vertical: componentes lógicos separados en máquinas separadas.\nArquitecturas simétricas # Distribución horizontal: dividir cada componente lógico (servidor, cliente).\npeer to peer (p2p): las funcionalidades del sistema están presentes en todos los procesos que constituyen el sistema. Un nodo puede actuar tanto como cliente o como servidor. La arquitectura del sistema toma la forma de una red sobrepuesta, que puede ser estructurada o no.\np2p estructurados # Los nodos se organizan según alguna topología concreta: anillo, árbol, matriz, etc. En general cada nodo es responsable de mantener un cierto conjunto de datos, identificados mediante un identificador (generalmente, una función hash de los datos). Así, la red p2p es básicamente un tabla hash distribuida. La topología define como debe realizarse el ruteo de una consulta al nodo correspondiente.\nEjemplo: chord\np2p no estructurados # No existe una topología predefinida y cada nodo mantiene una lista ad-hoc de nodos vecinos, lo que da como resultado un grafo aleatorio. Al momento de unirse, un nodo contacta un nodo bien conocido para obtener una lista inicial de vecinos. La búsqueda de datos requiere técnicas como inundación o random walks.\nInundación: se pasa la búsqueda a todos los nodos vecinos. Un nodo ignora una búsqueda que ya recibió. Puede responder al nodo que originó la búsqueda o al que se la reenvió. La búsqueda tiene un TTL asociado. TTL igual a 1 para buscar entre nodos vecinos.\nAleatorio: se pregunto a un nodo vecino al azar. Si no tiene el dato, este repite el procedimiento. Menor trafico, mayor tiempo de búsqueda. Se puede paralelizar y también tiene un TTL asociado.\nPor política: mantener una lista de nodos que han respondido peticiones, etc.\np2p jerárquicos # Para aliviar problemas de escalabilidad, un p2p no-estructurado puede tener nodos especiales que mantengan un índice de items o datos, conocidos como \u0026ldquo;super pares\u0026rdquo;.\nNodo \u0026ldquo;weak\u0026rdquo; se conecta a la red a travez de un super-par. Puede ser siempre el mismo o no. Para mejorar confiabilidad, puede requerirse conectarse a n \u0026gt; 1 superpares. Los nodos superpares se organizan en una red p2p propia (de ahí la jerarquía) Problemas: ¿Cómo elegir que nodo superpar utilizar? ¿Cómo elegir cuales seran superpares? Elección de lider. Ejemplo: bittorrent, CDN.\nArquitecturas híbridas # En la práctica, un sistema complejo abarca múltiples arquitecturas.\nCloud computing # Permite el acceso a un conjunto de recursos virtualizados de fácil acceso. Cúantos de estos recursos son necesarios y cómo serán usados, es definido dinámicamente: por ejemplo, si un cliente requiere más poder de cómputo, simplemente puede pedir procesador virtuales adicionales. Básicamente, una nube se organiza en cuatro capas:\nHardware: la capa más baja, que los clientes generalmente no ven, organizada en data-centers. Infraestructura: una capa de virtualización sobre los recursos de hardware, para ofrecerlos a los clientes. Plataforma: provee una capa de abstracción para la ejecución de aplicaciones y/o administración de recursos como almacenamiento. Aplicación: aplicaciones que pueden ser adaptadas por los clientes, como suites ofimáticas. Estas capas son accesibles mediante una multitud de interfaces (web-services, APIs, etc).\nA su vez, da lugar a tres capas de servicios diferenciados:\nIasS (Infraestructure-as-a-service) PaaS (Platform-as-a-service) SaaS (Software-as-a-service) FaaS (Function-as-a-service): ejemplo AWS Lambda. El uso de un servicio en la nube tiene similitudes con la arquitectura cliente-servidor. Sin embargo, el servidor es totalmente opaco al cliente: no se sabe donde ejecuta, como esta implementado, etc.\nEdge computing # Como colocar los recursos en el \u0026ldquo;límite\u0026rdquo; de la red, entre los dispositivos y la nube. Generalmente, en los ISPs. Complementa cloud para reducir la latencia y el uso de ancho de banda. También permite aumentar la confiabilidad, y puede ser necesario para cumplir con políticas de privacidad y seguridad. Aumenta la complejidad de la administración de la aplicación.\nEj: Akamai, Netflix CDN, IoT, etc.\nArgumentos a favor de edge computing:\nLatencia y ancho de banda: aunque el ancho de banda se ha incrementado en los últimos años, contar con los recursos más cerca del usuario final permite mejores garantías acerca del ancho de banda negociado. En cambio, la latencia es un problemas más complicado y donde existe un límite físico. En este caso, la cercanía reduce la latencia.\nConfiabilidad: Existen ciertos casos donde se debe garantizar el funcionamiento aún ante falta de conectividad a la nube.\nSeguridad y privacidad: por razones politicas/regulatorias ciertos datos no pueden ser subidos a la nube.\nDesafios en orquestación:\nRecursos: para garantizar la disponibilidad de recursos, ¿como deben ser asignados o provisionados?\nUbicación: Dónde y cúando los recursos deben ser hechos disponibles.\nSelección: no necesariamente el nodo más cercano es el mejor.\nBlockchain apps # Una presunción en el diseño de estas aplicaciones es que ningún nodo es confiable. Las transacciones son registras por un gran número de nodos participantes. Entre todos los nodos se mantiene una sola cadena de transacciones validadas. Dado que cada bloque es inmutable, la estructura de datos es fácilmente replicable.\nLa diferencia fundamental entre múltiples implementaciones es cuales nodos se encargan de realizar las validaciones (esto es, agregar nuevos bloques a la cadena). Agregar un bloque es básicamente llegar a un consenso entre los distintos nodos con rol de validador. Los tipos de consenso pueden ser:\nCentralizado: posible, pero en contra del diseño del sistema.\nDistribuido: un grupo preseleccionado de nodos se encarga de este rol.\nLos nodos llegan a un consenso, tolerando así participantes maliciosos. Si hay n nodos validadores, se toleran hasta k \u0026lt;= (n - 1)/3 nodos maliciosos/defectuosos. El problema es que en general n es un numero pequeño. Descentralizada: todos los nodos participantes llegan a un consenso.\nMediante consenso todos los nodos escogen un nodo que llevara adelante la validación. Este validación puede ser premiada. No todos los nodos desearan ser elegibles, principalmente por costo de la validación. "},{"id":2,"href":"/docs/03-procesos/","title":"Procesos","section":"Docs","content":" Procesos # Hilos, virtualización, cliente/servidor, migración.\nProcesos # Un proceso ofrece mejor isolación entre tareas.\nSobre todo, la protección puede ser asegurada por el hardware.\nHilos # Los hilos nos permiten mantener la idea de procesos secuenciales que se bloquean por E/S junto con concurrencia:\nUn mismo proceso puede tener multiples hilos, cada uno visto como un programa secuencial. Evita tener que lidiar con callbacks asincrónicos. El uso de hilos provee un mejor nivel de granularidad para el desarrollo de aplicaciones distribuidas y provee un mejor nivel de performance.\nA diferencia de los procesos, los hilos no ofrecen transparencia de concurrencia:\nComparten el mismo segmento de memoria. Mayor dificultad al diseñar la aplicación. Implementación:\nNivel de usuario: creación/destrucción y cambio de contexto menos costoso; bloqueo de todo el proceso. Nivel de kernel: administración y sincronización ligeramente más costosa, pero sin bloqueo de todo el proceso. En un sistema distribuido, el uso de hilos permite simplificar la comunicación entre nodos, al implementar esta funcionalidad en un hilo separado en el programa.\nEjemplo:\nUn cliente multi-hilo permite ocultar latencias, mejorando así la transparencia de distribución. Un navegador web permite visualizar una página aún cuando no se haya descargado completamente. La mayor ventaja se puede observar en la implementación de servidores concurrentes.\nAlternativas:\nun solo hilo (secuencial) uso de máquinas de estado (más complejo). Virtualización # El software por lo general \u0026ldquo;sobrevive\u0026rdquo; al hardware.\nMediante la virtualización es posible reemplazar una interface particular de un sistema (sea hardware o software) por otra implementación que copia exactamente su comportamiento.\n¿Por qué es utilizada?\nPermitir que software ejecute en nuevas arquitecturas. Simplifica el soporte de aplicaciones del lado del servidor: no requiere mantener multiples máquinas fisicas. Portabilidad y flexibilidad: migración de maquinas virtuales enteras de cloud a edge, por ejemplo. Seguridad: aislar codigo en ejecucion a un entorno virtual. Tipo de virtualización # Hardware: instrucciones hardware de la plataforma (privilegiadas y no-privilegiadas). Llamadas al sistema: ofrecidas por el sistema operativo. API: ofrecida por la aplicación/librería. Contenedores # Permitir que múltiples aplicaciones ejecuten de manera aislada, cada una con su propio ambiente de software (librerías, sistema operativo, etc).\nUso de maquinas virtuales en ssdd # En la computación en la nube, la virtualización es fundamental para ofrecer servicios IaaS (Infraestructure as-a service) Clientes # En un sistema distribuido, el software ejecutando en el cliente busca por lo general mejorar la transparencia de distrbución\nAcceso. Ubicación. Migración. Reubicación. Replicación. Falla. Concurrencia. Servidores # Un servidor implementa una serie de servicios que ejecuta a pedido de un cliente. El servidor espera por una petición, la procesa y retorna el resultado.\nEl servidor puede estar organizado como:\nIterativo Concurrente Los clientes se contactan con el servidor haciendo uso de un endpoint. Generalmente, una IP:PUERTO. Estos pueden ser bien conocidos o requerir un lookup previo por parte del cliente.\nSegún la información de estado que administre, un servidor puede ser:\nSin estado: la información del cliente se descarta cuando finaliza la conexión / operación. Con estado: mantiene información persistente de sus clientes. Servidor de objetos # El servidor no ofrece una funcionalidad específica, en cambio esta es provista por los objetos que almacena.\nCluster de servidores # Lan Cluster # Un conjunto de servidores interconectados por una red de alta velocidad.\nOrganización general:\nPrimer capa: switch que redirecciona las peticiones Segunda capa: conjunto de nodos que ejecutan la logica de negocio/servicio Tercer capa: nodos que se encargan de persistencia de datos La redirección de peticiones puede realizarse en\nnivel de transporte (ej TCP) nivel de aplicación (en base al contenido del request) Wan Cluster # Uso de DNS para el balanceo de carga o reducción de latencia. Ejemplo: Content Delivery Networks (CDNs) como Akamai Migración de código # Razones # Perfomance\nMover código/procesos a nodos con menor carga. Actualmente se busca mover procesos de nodos con poca carga a otros con carga media/alta. Estrategia de optimización de consumo de energía en datacenters. Migración de VMs menos compleja (aunque requiere más recursos) que migración de procesos. En un sistema distribuido puede importar más reducir latencias que mejorar perfomance de ejecución. La heterogenidad de plataformas dificulta determinar ganancia de computo. Seguridad y privacidad\nMover el código/procesos a donde esten los datos por cuestiones de seguridad/privacidad de los mismos. Por ejemplo, mover procesos para entrenar una red neuronal a los nodos con los datos, no al reves. Flexibilidad\nPermitir reconfigurar dinámicamente un sistema distribuido. Por ejemplo, descargar implementación de un protocolo dinámicamente para comunicación con un servidor. Modelos para migración de código # Ademas de código, puede ser necesario mover datos y el contexto de ejecución (migración de procesos) Podemos identificar en un proceso: código, recursos y contexto de ejecución. La migración se puede clasificar como sender-initiated: ej, enviar un programa a un servidor para su ejecución receiver-initiated: recepcionar un codigo, ejemplo un applet o javascript Como resultado, tenemos cuatro modelos: Cliente-Servidor: todo se ejecuta en el servidor, solo se envia un resultado. Evaluación Remota: código migra al servidor, donde se ejecuta. Codigo a demanda: código migra al cliente, donde se ejecuta. Agentes móbiles: se migra código y contexto de ejecución. Se puede diferenciar tambien: Movilidad débil: solo se migra código (requiere crear un nuevo ambiente de ejecución), modelo sencillo. Movilidad fuerte: se migra código y ambiente de ejecución (ej, migración de un proceso en ejecución, o clonado). Heterogeneidad # Los sistemas distribuidos son heterogéneos No siempre se puede migrar código y ejecutarlo en el nodo receptor, por diferencias en arquitectura, software, etc. Solución: interpretes y/o máquinas virtuales Ejemplos de lenguajes: Python, Java, Pascal, Erlang/Elixir, etc. Migración de todo el entorno: VMs como Dropbox, VMWare, VirtualPC, etc. "},{"id":3,"href":"/docs/04-01-protocolos/","title":"Protocolos de comunicación","section":"Docs","content":" Comunicación # Fundamental en un sistema distribuido. Las primitivas de comunicación que ofrece el sistema operativo pueden no tener el nivel de abstracción necesario.\nFundaciones # El modelo OSI # Modelo de siete capas, no utilizado en la práctica, pero que es una referencia útil acerca de como esta estructurado lógicamente el stack de comunicación.\nMiddleware # Los servicios middleware para un sistema distribuido estarían logicamente ubicados en las capas de sesión y presentación del modelo OSI, aunque también pueden incorporar servicios en la capa de aplicación.\nTipos de comunicación # El middleware puede ser visto como un servicio adicional que media en la comunicación en una arquitectura cliente/servidor.\nLos tipos de comunicación se pueden categorizar en:\nPersistente:\nEl mensaje es almacenado por el middleware todo el tiempo que sea necesario para realizar la entrega. El emisor no necesita esperar a que se complete la recepción. El receptor no tiene por que estar ejecutando al ser enviado el mensaje. Transitoria:\nEl mensaje es almacenado únicamente el tiempo suficiente para el envío, sólo si emisor y receptor estan ejecutando. Cualquier error descarta el mensaje. Asincrónica:\nEl emisor continua con la ejecución luego de envíar el mensaje, quiza sin confirmación de envío ni recepción. Sincrónica:\nEl emisor se bloquea hasa que el mensaje sea aceptado (envío, recepción, procesamiento). RPC # Los desarrolladores estan familiarizados con el paradigma procedural. Si un procedimiento esta diseñado de manera que funcione aislado, no hay impedimento en principio que pueda ejecutar en otra maquina.\nIdea básica de RPC: Permitir invocar funciones remotas como si fueran locales.\nIdea sencilla pero de implementación compleja. Contribuye a la transparencia de distribución, especialmente a la transparencia de acceso. Problemas: falta de un espacio de direcciones común, diferencia en arquitecturas, caída de alguno de los procesos que se comunican, etc.\nEl proceso cliente invoca una función local que se denomina stub, que tiene la misma sintaxis que la función remota deseada, pero que se encarga de agrupar los parámetros en un mensaje y enviarlo al servidor, esperar la respuesta y desampaquetar los datos y retornar el resultado de la invocación.\nEn el servidor ocurre algo análogo: una funcion stub recibe la petición, desempaqueta los parámetros e invoca la función local en el servidor, y luego envía la respuesta a al cliente.\nPaso de parámetros # Aspecto dificultoso del esquema RPC:\n¿Cómo interpretar los párametros? ¿Cómo asegurar la misma representación de los datos? Existen diferencias en las arquitecturas, por ejemplo ordenamiento de los bytes o tamaño de palabras. Distintos lenguajes tiene diferentes tipos de datos. Solución: enviar datos en un formato independiente de la maquina.\nPor ejemplo, se utiliza big endian para ordenar los bytes en los mensajes en la red. Acuerdo en la codificación de tipos basicos y complejos. ¿Cómo pasamos punteros?\nProhibirlos (no es realista) Serializar toda la estructura de datos (por ejemplo el arreglo, lista, etc) Generalmente se puede utilizar un handle. Por ejemplo, nombre de archivo o url. Soporte # Dos alternativas:\nEspecificar detalladamente funciones y parametros, para generar stubs.\nIndicar como empaquetar el nombre de la función y sus parámetros. Representación de los tipos de datos. Decidir en el mecanismo de comunicación, por ejemplo mediante TCP/IP. La interface es especificada mediante un IDL (Interface Definition Language). Mediante un programa específico, la descripción mediante IDL es compilada en stubs. Incorporar la funcionalidad en el lenguaje de programación.\nFacilita el desarrollo de la aplicación. Ej: Java cuenta con RMI (Remote Method Invocation) Descubrimiento:\n¿Cómo puede el cliente saber qué servidor implementa la funcionalidad requerida? Solucion 1: el servidor puede ser bien conocido. Solucion 2: usar un servicio de directorio: El servidor registra en un directorio el servicio que ofrece y su dirección. El cliente contacta el directorio y consulta por un servicio en particular. El cliente se conecta al servidor que le indica el directorio. Variantes # RPC sincrónico: el emisor espera a que el receptor ejecute la función. RPC asincrónico: el emisor sólo espera hasta la confirmación de recepción por parte del emisor. RPC diferido: RPC asincrónico más un callback que se ejecuta al recibir la respuesta del receptor. Alternativamente al callback, el cliente puede realizar un polling. one-way RPC: el emisor genera el RPC pero no espera ni siquiera la confirmación de recepción. RPC Multicast: uso de one-way RPC para enviar múltiples peticiones, posiblemente con un callback. La aplicación puede no conocer que se realiza un multicast, lo oculta el stub. Es posible que tampoco lo sepa el stub, si se realiza mediante multicast en la capa de transporte. ¿Cómo procesar las respuestas? ¿La primera unicamente, todas? Depende de la aplicación. "},{"id":4,"href":"/docs/04-02-rpc/","title":"RPC","section":"Docs","content":" RPC # Los desarrolladores estan familiarizados con el paradigma procedural. Si un procedimiento esta diseñado de manera que funcione aislado, no hay impedimento en principio que pueda ejecutar en otra maquina.\nIdea básica de RPC: Permitir invocar funciones remotas como si fueran locales.\nIdea sencilla pero de implementación compleja. Contribuye a la transparencia de distribución, especialmente a la transparencia de acceso. Problemas: falta de un espacio de direcciones común, diferencia en arquitecturas, caída de alguno de los procesos que se comunican, etc.\nEl proceso cliente invoca una función local que se denomina stub, que tiene la misma sintaxis que la función remota deseada, pero que se encarga de agrupar los parámetros en un mensaje y enviarlo al servidor, esperar la respuesta y desampaquetar los datos y retornar el resultado de la invocación.\nEn el servidor ocurre algo análogo: una funcion stub recibe la petición, desempaqueta los parámetros e invoca la función local en el servidor, y luego envía la respuesta a al cliente.\nPaso de parámetros # Aspecto dificultoso del esquema RPC:\n¿Cómo interpretar los párametros? ¿Cómo asegurar la misma representación de los datos? Existen diferencias en las arquitecturas, por ejemplo ordenamiento de los bytes o tamaño de palabras. Distintos lenguajes tiene diferentes tipos de datos. Solución: enviar datos en un formato independiente de la maquina.\nPor ejemplo, se utiliza big endian para ordenar los bytes en los mensajes en la red. Acuerdo en la codificación de tipos basicos y complejos. ¿Cómo pasamos punteros?\nProhibirlos (no es realista) Serializar toda la estructura de datos (por ejemplo el arreglo, lista, etc) Generalmente se puede utilizar un handle. Por ejemplo, nombre de archivo o url. Implementación # Dos alternativas:\nEspecificar detalladamente funciones y parametros, para generar stubs.\nIndicar como empaquetar el nombre de la función y sus parámetros. Representación de los tipos de datos. Decidir en el mecanismo de comunicación, por ejemplo mediante TCP/IP. La interface es especificada mediante un IDL (Interface Definition Language). Mediante un programa específico, la descripción mediante IDL es compilada en stubs. Incorporar la funcionalidad en el lenguaje de programación.\nFacilita el desarrollo de la aplicación. Ej: Java cuenta con RMI (Remote Method Invocation) Descubrimiento # ¿Cómo puede el cliente saber qué servidor implementa la funcionalidad requerida? Solucion 1: el servidor puede ser bien conocido. Solucion 2: usar un servicio de directorio: El servidor registra en un directorio el servicio que ofrece y su dirección. El cliente contacta el directorio y consulta por un servicio en particular. El cliente se conecta al servidor que le indica el directorio. Variantes # RPC sincrónico: el emisor espera a que el receptor ejecute la función. RPC asincrónico: el emisor sólo espera hasta la confirmación de recepción por parte del emisor. RPC diferido: RPC asincrónico más un callback que se ejecuta al recibir la respuesta del receptor. Alternativamente al callback, el cliente puede realizar un polling. one-way RPC: el emisor genera el RPC pero no espera ni siquiera la confirmación de recepción. RPC Multicast: uso de one-way RPC para enviar múltiples peticiones, posiblemente con un callback. La aplicación puede no conocer que se realiza un multicast, lo oculta el stub. Es posible que tampoco lo sepa el stub, si se realiza mediante multicast en la capa de transporte. ¿Cómo procesar las respuestas? ¿La primera unicamente, todas? Depende de la aplicación. "},{"id":5,"href":"/docs/04-comunicacion-mom/","title":"MoM","section":"Docs","content":" Middleware orientado a mensajes (MoM) # RPC o RMI no siempre son apropiados. Ej:\nel receptor no esta funcionando al mismo tiempo que el emisor. no se ajustan a la arquitectura cliente/servidor. Alternativa: envío de mensajes.\nUso de sockets # Socket: abstraccion sobre un puerto, donde se puede escribir o leer datos, usando un protocolo específico (ej: TCP o UDP).\nNo presenta el nivel de abstracción necesario. Cualquier funcionalidad adicional debe ser implementada por la aplicación.\nUso de patrones # La mayoría de las comunicaciones realizadas por las aplicaciones pueden ser categorizadas en unos pocos patrones:\nEj: request-reply, publish-subscribe, pipeline\nEj de implementación: ZeroMQ.\nMPI # Uso de paso de mensajes en computación de alto perfomance, por ejemplo clusters. TCP esta orientado a su uso sobre IP, por lo cual no es necesariamente efectivo en estas situaciones.\nEl estándar MPI se definio para lograr interoperabilidad entre soluciones de paso de mensajes para este tipo de escenarios.\nEj: no asume que un error en la red es recuperable.\nConsidera grupos de procesos, donde cada proceso tiene un identificador (grupoID, procesoID). Un proceso puede pertenecer a mas de un grupo.\nMas de 650 operaciones definidas.\nComunicación persistente # Sistemas de manejos de colas: ofrecen soporte para la comunicación asincrónica persistente.\nIdea básica: las aplicaciones se comunican enviando mensajes a buzones. Estos mensajes pueden a su vez ser reenviados a otros servidores de colas. En general cada aplicación tiene asociada una cola de mensajes.\nGarantía: en general se da la garantía que el mensaje será puesto en la cola de mensajes del receptor, pero no que este lo leerá.\nEmisor y receptor quedan así totalmente desacoplados en tiempo y espacio.\nEl contenido de los mensajes es arbitrario, aunque posiblemente limitado en tamaño. Solamente debe estar correctamente indicado el receptor.\nPrimitivas: PUT, GET, POLL, NOTIFY.\nArquitectura de un MoM # En general las colas de mensajes son responsabilidad de un administrador de colas de mensajes (queue manager).\nEn general el administrador de colas de mensajes es un proceso separado del cliente y/o el emisor.\nEl administrador tiene la responsabilidad de \u0026ldquo;rutear\u0026rdquo; los mensajes correctamente.\nEn general las direcciones de las colas de mensajes deben proveer transparencia de ubicación.\nUna cuestión a tener en cuenta es cómo informar a los distinos administradores de las direcciones existentes.\nEn sistemas complejos, no es realista que un administrador conozca a todo el resto: se debe rutear los mensajes con información incompleta (problema analogo a los routers en una red IP). Se utiliza una red superpuesta.\nBrokers # Un uso común de los sistemas de mensajes es integrar aplicaciones nuevas y existentes en un sistema coherente (¿suena?)\nLa integración requiere que las aplicaciones comprendan los mensajes que reciben del resto.\nEsto requiere que cada aplicacion entienda la sintaxis y la semántica de los protocolos utilizados por el resto.\nSoluciones?\nCada aplicación convierte los mensajes: impráctico, en un sistema con N aplicaciones, se requieren N^2 convertidores. Protocolo común: no es realista, dada la heterogeneidad de las aplicaciones. Información de sintaxis en cada mensaje: ejemplo, con esquemas XML. Falta entender la semántica. Entonces? No se puede esconder la situación, por lo tanto se debe ofrecer un mecanismo lo más simple posible para las conversiones.\nUn broker es una aplicación en un sistema de mensajería que se encarga de la conversión de mensajes.\nMucho más que un convertidor, puede actuar también como un gateway a nivel de aplicación:\nManejo de publicación/subscripción. Prioridades. Multicasting. Logging. Balanceo de carga. Etc. Para todo esto, un broker maneja una serie de reglas de transformación, ruteo, etc., que deben ser configuradas por un experto.\nEjemplo: AMQP # Advanced Message-Queuing Protocol (AMQP).\nAMQP ofrece:\nUn servicio de mensajería. Un protocolo de mensajes. Una interfaz de mensajería para las aplicaciones. Comunicación:\nUna aplicación establece una conexión con el administrador de colas de mensajes. Una conexión incorpora múltiples canales de una sola vía. Conexión -\u0026gt; mayor tiempo de vida, estable Canal -\u0026gt; dinámica, tiempo de vida breve Sesión: agrupamiento lógico de dos canales para comunicación full-duplex Manejo de mensajes:\nTipos de nodos: productor, consumidor, cola Los mensajes pueden ser persistentes (los nodos intermedios deben poder recuperarlo luego de un error) "},{"id":6,"href":"/docs/04-comunicacion-multicast/","title":"Multicast","section":"Docs","content":" Multicast # ¿Cómo enviar datos a múltiples receptores?\nExisten numerosas soluciones a nivel de protocoles de red y de transporte. Su principal desventaja es el costo en armar las rutas de difusión de datos.\nA nivel de aplicación, las redes p2p estructuradas facilitan la creación de estas rutas de difusión. Veremos técnicas de difusión a este nivel.\nBasada en árboles # La idea básica es que los nodos estan organizados en una red superpuesta, utilizada para difundir los datos.\nLas conexiónes lógicas pueden no ser óptima desde el punto de vista de los enlaces físicos.\nExisten básicamente dos alternativas para la topología:\nArbol: existe un único camino entre dos nodos cualesquiera de la red. Mesh: cada nodo tiene múltiples vecinos y por lo tanto requiere algún tipo de ruteo (existe más de un camino entre dos nodos cualesquiera) Principal diferencia: mesh ofrece mayor tolerancia a fallos que árbol.\nPrincipal desafío: ¿cómo construir la red superpuesta para la difusión?\nAdicional: ¿cómo construimos un árbol de difusión eficiente?\nLa calidad del árbol para multidifusión se puede medir con tres métricas:\nLink stress: ¿cuántas veces debe un paquete atrevesar el mismo enlace? Link stretch: la razón entre el número de saltos en la red superpuesta y los enlaces físicos. Tree cost: métrica global, como reducir el costo agregado de los enlaces. Situación: un nuevo nodo quiere sumarse a la red superpuesta.\nSe contacta con un nodo bien conocido. ¿Cómo decidir que nodo será su nodo padre en el árbol? Para evitar sobrecargar nodos, en general se pone un límite k de nodos vecinos. Este límite dificulta establecer el árbol, ya que agregar un nodo puede requerir una reconfiguración. Inundación (flooding) # Para minimizar el número de nodos que reciben un mensaje del cual no son destinatarios, es mejor construir una red superpuesta con los nodos destino.\nEj: si en una topología de árbol un mensaje solo es para los nodos hoja.\nPosible solución: diferentes redes superpuestas para cada grupo multicast. Desventaja: un nodo puede pertencer a varias redes superpuestas, lo que incrementa el costo de administración.\nUna técnica sencilla de diseminar información a todos los nodos es la inundación:\nEnviar el mensaje a todos los nodos vecinos, excepto de quien lo recibió. Si se mantiene referencia de los mensajes enviados, se puede evitar duplicados. Problema: ineficiente (gran cantidad de mensajes). Sólo sería eficiente si la red superpuesta fuera un árbol.\nSe puede mejorar la situación utilizando inundación probabilistica:\nUn nodo reenviara el mensaje m a un nodo vecino con una probabilidad p. El número total de mensajes decrece de manera lineal con p. Problema: A menor valor de p, más chances que existan nodos que no reciban el mensaje. Se puede entonces tener en cuenta también el número de nodos vecinos al momento de decidir si reenviar el mensaje o no. ¿Y si la red superpuesta tiene una topología estructurada? Las cosas se hacen más fáciles.\nEjemplo: hipercubo. Reenviar mensajes a nodos con una dimension superior. Total de mensajes: 2^n - 1.\nOtro ejemplo: chord.\nEpidemico # Diseminar información siguiendo un comportamiento similar a los contagios de enfermedades. Como \u0026ldquo;infectar\u0026rdquo; rapidamente todos los nodos con un nuevo dato.\nIdea: difundir rápidamente información utilizando únicamente información local a cada nodo.\nVentaja: es una técnica escalable, requiere pocas sincronizaciones entre nodos.\nSuponemos que las actualizaciones se inician en un único nodo.\nTerminología:\nInfectado: nodo que tiene un dato que desea transmitir. Susceptible: nodo que no ha visto aún este nuevo dato. Removido: nodo que no reenvia datos. Modelos de propagación: antientropía y rumores\nAntientropia # Un nodo P eligue al azar un nodo vecino Q para intercambiar datos.\nP puede sólo envíar datos a Q (push) P puede sólo requerir datos de Q (pull) P y Q intercambian datos (push-pull) Sólo utilizar push no es eficiente si existen muchos nodos infectados: la probabilidad de escoger un nodo susceptible es baja. Usar pull es conveniente cuando el número de nodos infectados es alto. Por lo tanto, la mejor estrategia es push-pull.\nRonda: intervalo de tiempo en el cual cada nodo intercambio datos con un nodo vecino al azar.\n¿Cuántas rondas se necesitan para difundir a todos los nodos una actualización? Orden: O(log(N))\nRumores # Variante de epidémico: si el nodo P contacta un nodo Q al azar para comunicar el dato x. Si Q ya conoce el dato, P dejará de transmitir el dato (con una probabilidad p).\nVentajas: difunde muy rapidamente las actualizaciones. Desventaja: probabilidad de que no todos los nodos sean contactados.\nIncluso con valores bajos de p existe la posibilidad de que algunos nodos no sean actualizados. Para valores altos de p se deben tomar acciones adicionales en caso de que se requiera que la mayoría o todos de los nodos sean actualizados.\nRumores dirigidos # Una presunción que se hace es que un nodo P puede contactar cualquier nodo Q de la red. Esto raramente es así (no se cuenta con una lista completa de los nodos).\nEliminar datos # Los algoritmos epidemicos son excelentes para difundir una actualización.\nProblema: es muy complicado difundir una eliminación.\nSi un nodo elimina el datos x y posteriormente recibe una mensaje viejo de actualización, lo interpretará como un dato nuevo.\nSolución: realizar borrados lógicos, reenviando certificados de defunción.\nProblema: acumulación de certificados.\nSi se sabe que el tiempo de propagación de una actualización es n, se puede eliminar un certificado luego de n\u0026hellip; pero por las dudas, ciertos nodos específicos mantienen copias de estos certificados.\n"},{"id":7,"href":"/docs/05-01-relojes/","title":"Relojes","section":"Docs","content":" Relojes # En un sistema centralizado x = timestamp(); y = timestamp() da como resultado \\( x \\leq y \\).\nEn un sistema distribuido, acordar en un valor temporal no es trivial.\n¿Es posible sincronizar los relojes de los nodos de un sistema distribuido? La respuesta es sorprendentemente complicada.\nEntonces\u0026hellip; ¿Cómo coordinan sus actividades los procesos de un sistema distribuido?\nRelojes físicos # Existen situaciones donde es necesario que todos los nodos en un sistema acuerden en un valor de tiempo determinado.\nUn reloj físico presenta deriva\nProblemas:\n¿Cómo sincronizamos el reloj interno con un reloj externo? ¿Cómo sincronizamos los relojes internos entre sí? UTC # Coordinated Universal Time: estándar internacional\n40 emisoras de onda corta difunden una señal al comienzo de cada segundo UTC\nPrecisión $\\pm 1$ ms a $\\pm 10$ ms Uso de satelites, por ejemplo GPS y relojes atómicos\nPrecisión $\\pm 0,5$ ms Sincronización de relojes # Suponer un conjunto de nodos:\nDesafio 1: que esten sincronizados con una referencia externa, por ejemplo UTC. Desafio 2: que los relojes de los nodos difieran lo menos posible. Exactitud: mantener la desviación con respecto a una fuente externa dentro de un rango específico. Precisión: mantener la desviación entre dos relojes dentro de un rango específico.\nSincronización externa: mantener los relojes exactos.\nSincronización interna: mantener los relojes precisos.\nProblema:\nLos relojes fisicos no son exactos, presentan deriva El reloj por software se basa en el reloj hardware Segun la deriva un reloj puede ser más rapido o más lento en referencia a un reloj ideal Fun facts:\nDos relojes exactos pueden ser precisos con una cota $2 \\delta$ Sin embargo, ser precisos no indica nada acerca de la exactitud. Servidor de tiempo:\nObtener un valor actualizado desde un servidor central (por ejemplo, que tenga un reloj UTC) Implementación mediante una arquitectura cliente/servidor: Servidor retorna al cliente respuesta con timestamp. Cliente debe compensar el offset y delay. NTP # Network Time Protocol\nAmbientes inalambricos: # Requieren algoritmos diferentes\nEjemplo: Reference Broadcast Synchronization\nRelojes lógicos # Generalmente lo que importa es que los nodos esten de acuerdo en el orden de los eventos.\nRelación happened-before # $a$ y $b$ son eventos Si $a$ ocurre antes que $b$ en un mismo proceso, entonces $a \\rightarrow b$ Si $a$ es el envío de un mensaje y $b$ la recepción, entonces $a \\rightarrow b$ Transitivo: si $a \\rightarrow b$ y $b \\rightarrow c$, entonces $a \\rightarrow c$ Introduce un ordenamiento parcial sobre los eventos. Diseño # Cada evento $e$ tiene asociado un timestamp $C(e)$ Propiedad 1: si $a$ y $b$ son eventos en un mismo proceso y $a \\rightarrow b$ entonces $C(a) \u0026lt; C(b)$ Propiedad 2: si $a$ y $b$ son envío y recepción de un mensaje respectivamente, entonces $C(a) \u0026lt; C(b)$ Implementación # Cada proceso $P_i$ mantiene un reloj lógico $C_i$ Por cada evento en $P_i$, $C_i = C_i + 1$ Cada mensaje enviado por Pi tiene un timestamp ts(m) = C(i) Cuando Pj recibe un mensaje m: Ajusta su contador Cj con el máximo valor entre Cj y ts(m) Antes de pasar el mensaje a la aplicación, incrementa Cj en uno. El servicio de relojes lógicos es implementado en un middleware, idealmente la aplicación no tiene por que ocuparse.\nEjemplo # P1 envía a P2 P2 envía a P3 P3 envía a P1 Multicast totalmente ordenado # Es posible realizar un multicas totalmente ordenado mediante el uso de relojes lógicos:w\nRelojes vectoriales # Al usar relojes lógicos si $C(a) \u0026lt; C(b)$ no implica que $a$ ocurra antes que $b$\nSolución: relojes vectoriales\nCada nodo mantiene su propio reloj lógico Mantiene información de los relojes lógicos del resto de los nodos Se organizan como un vector o arreglo $V[i]$ es el reloj lógico del nodo $i$ $V[j]$ es el reloj lógico del nodo $j$ Ahora el timestamp asociado a cada mensaje es un vector: ts(m) = VC\nPara comparar dos relojes vectoriales, VCa \u0026lt; VCb si y solo si:\nVCa[k] \u0026lt;= VCb[k] para cualquier k existe al menos un k\u0026rsquo; tal que VCa[k\u0026rsquo;] \u0026lt; VCb[k\u0026rsquo;] Si se cumple VCa \u0026lt; VCb entonces un evento precede causalmente a otro.\nImplementación: similar al reloj lógico\nEjemplo de uso: multicast totalmente ordenado.\nEmisor: P1 P2 P3 Receptor: P1 P2 P3 Enviar Mensaje "},{"id":8,"href":"/docs/05-02-mutex/","title":"Mutex","section":"Docs","content":" Exclusión mutua # Coordinar el acceso exclusivo a un recurso.\nEstrategias:\nMediante permisos: acuerdo entre los procesos. Utilizando un token: quien tiene el token, accede al recurso. Centralizado # Uso de un coordinador que otorga el acceso al recurso Facil de implementar, sencillo de mantener. Posibles problemas para escalar. Descentralizado # Recurso con N replicas, cada una con un coordinador asignado. Acceder al recurso requiere votos positivos de al menos m \u0026gt; N/2 coordinadores. Distribuido # Uso de multicast totalmente ordenado para coordinar el acceso. Requiere poder contactar a todos los procesos interesados en el mismo recurso. Token ring # Procesos organizados en un anillo lógico. Token circula por el anillo. Quien tiene el token puede acceder al recurso. Comparación # Centralizado: Requiere 3 mensajes para acceder/liberar el recurso (petición, recepción del ok, liberación). Distribuido: Si existen N nodos, debo envíar mensajes a cada uno y esperar confirmación de ok: 2(N-1) mensajes. Token-ring: El token puede recorrer indefinidamente el anillo hasta ser retenido para el acceso al recurso. En el peor caso un nodo debe esperar N-1 mensajes hasta que le llegue el token (suponiendo un anillo de N nodos) Ejemplo: exclusion mutua con Zookeeper # En la práctica muchos sistemas distribuidos utilizan un coordinador centralizado\nZookeeper ofrece servicios para:\nexclusion mutua elección de lider monitoreo etc Diseñado para ofrecer confiabilidad, tolerancia a fallas y escalabilidad\nAunque logicamente centralizado, su implementación es un sistema distribuido Usar zookeeper o servicios similares: ¡no hay que reinventar la rueda! (sobre todo una rueda complicada)\nZookeeper 101:\nNo hay primitivas bloqueantes Las peticiones de un cliente siempre reciben una respuesta. Ofrece un espacio de nombres, similar a un sistema de archivos. Operaciones: crear y eliminar nodos leer y actualizar datos en nodos (las actualizaciones son completas, no parciales) verificar si existe un nodo en particular Tipos de nodos: persistentes: deben ser creados y eliminados explicitamente efímeros: son eliminados cuando la conexión del proceso que los creo se pierde Servicio de notificaciones Evita polling por parte de los clientes. Ejemplo: obtener acceso exclusivo a un recurso\nUn proceso crea un nodo, por ejemplo con nombre /lock Si existe, la operación falla indicando que ya existe El proceso debe repetir la operación para obtenerlo En caso de crearlo, para liberar el acceso elimina el nodo /lock Problemas: ¿que sucede cuando un cliente crea /lock y desaparece? Proceso p2 puede solicitar notificaciones por /lock mientras /lock es eliminado Estas sutilezas y muchas más son manejadas por zookeeper. "},{"id":9,"href":"/docs/05-03-eleccion/","title":"Algoritmos de elección","section":"Docs","content":" Algoritmos de elección # Muchos algoritmos distribuidos requieren que un nodo actue como coordinador.\nNo importa en general cual nodo en particular sea el coordinador\u0026hellip; pero alguien tiene que hacerlo.\nMediante un algoritmo de elección se escoje un nodo para que actue como coordinador.\nEn general se asume:\nCada proceso P cuenta con un identificador único id(P). Cada proceso conoce a todo el conjunto de procesos (aunque no cuales estan funcionando). El objetivo de estos algoritmos es que cuando finalice la elección todos los procesos hayan acordado el mismo lider.\nAlgoritmo del matón (bully) # Considerar N procesos, cada uno con un identificador k, con k entre 0 y n-1. Cuando un proceso k se da cuenta que el lider no responde: Envía un mensaje ELECTION a todos los nodos con identificador \u0026gt; k. Si ninguno responde, el nodo k asume el papel de líder. Si alguno responde con OK, toma el control del proceso de elección y k desiste. Eventualmente, sólo un proceso tomará el control, enviando el mensaje COORDINATOR. Si un proceso caído retoma su ejecución, inicia una elección. Como el proceso con mayor ID es el que gana, se lo conoce por el nombre de \u0026ldquo;bully algorithm\u0026rdquo;. Elección en un anillo # Suponer que cada nodo conoce su sucesor, y al siguiente a este, y al proximo, y así.\nCuando un nodo detecta que el coordinador no responde:\nEnvía un mensaje ELECTION a su sucesor (o al siguiente si este no responde), con su ID. El receptor reenvia el mensaje ELECTION, agregando su propio ID. Eventualmente, el mensaje retorna al emisor original. En ese momento, el mensaje circula nuevamente ahora con el tipo COORDINATOR. El mensaje contiene ahora: el nuevo coordinador (el ID mas alto) y que nodos estan activos en el anillo. ¿Importa que dos o más procesos inicien una elección?\nNo, únicamente habrá mayor recarga en la red. Elecciones en sistemas de gran escala # Muchos algoritmos de elección suponen un número pequeño de nodos.\nLas cosas se vuelven complicadas a medida que el número de nodos aumenta.\nUn ejemplo es una red blockchain.\nProof of work # Consiste en que los nodos compitan en base a su poder de cómputo\nPara esto, compiten para ver quien es el primero en resolver un problema complejo pero soluble.\nEl ganador es el nodo que primero difunde una solución.\nEl nodo ganador se convierte en el líder: es quien añade la transacción a la cadena de bloques.\nMultiples problemas:\nPrincipalmente, consumo de energía. ¿Cómo regular la complejidad del problema? Proof of stake # Elecciones en redes inalambricas # En una red inalambrica, la transmisión no es necesariamente confiable, ni la topología permanece estática.\nEl algoritmo presentado por Vasudevan escoge el mejor líder.\nPara elegir un líder un nodo difunde un mensaje ELECTION a sus vecinos.\nSi un nodo vecino hubiera recibido ya un mensaje ELECTION, simplemente retorna un ACK.\nCaso contrario, si recibe un mensaje ELECTION por primera vez recuerda al nodo emisor y retrasmite el mensaje.\nEn cuanto todos los nodos vecinos responda a esta retransmisión de ELECTION, el nodo responde al emisor original.\n"},{"id":10,"href":"/docs/05-04-agregacion/","title":"Coordinación mediante rumores","section":"Docs","content":" Coordinación basada en rumores # Se puede utilizar rumores para recolectar información.\nEjemplos de coordinación # Consensuar un mismo valor:\nCada nodo $P_i$ escoge un valor arbitrario $v_i$ Cuando dos nodos $P_i$ y $P_j$ intercambian datos: $v_i, v_j \\leftarrow (v_i + v_j)/2$ Eventualmente todos los nodos tendran el mismo valor (media de los valores iniciales) Estimar el número de nodos:\nEl nodo $P_1$ escoge $v_1=1$, el resto de los nodos $v_i=0$ Si hay $N$ nodos, eventualmente todos tendran $v_i=1/N$ Se puede estimar el tamaño de la red como $1/v_i$ Seleccionar un nodo al azar:\nCada nodo $P_i$ seleccionar un valor $m_i$ al azar y setea $v_i=m_i$ Al intercambiar datos $P_i$ y $P_j$ realizan $v_i, v_j \\leftarrow max{v_i, v_j}$ Si luego $m_i \u0026lt; v_i$, entonces el nodo $P_i$ pierde la competencia. Eventualmente un único nodo será el ganador. Eventuales aplicaciones # Un nodo al azar inicia el proceso de estimación de números de nodos. Si el numero de nodos es estable, se puede designar un nodo fijo para que realice el conteo. Caso contrario, se pueden utilizar epocas o bien un nodo al azar cada tanto realiza un conteo. Peer-sampling # ¿Cómo elegir un nodo al azar cuando no se conoce la totalidad de los nodos en el sistema?\nUn nodo podría tener toda la información, pero no es una solución escalable.\nUna solución es el uso de vistas parciales:\nCada nodo mantiene una lista de c nodos vecinos. Los nodos intercambian parte de sus listas parciales con otros nodos (en su vista parcial). Cada nodo actualiza su vista parcial, pero siempre manteniendo c nodos en la misma. Si esto se repite regularmente, escoger un nodo al azar de la vista parcial es estadisticamente indistinguible de hacerlo de la totalidad de los nodos.\nConstruccion de redes superpuestas # Es posible utilizar las vistas parciales para generar topologías estructuradas.\nUn posible protocolo para lograrlo estaría dividido en dos capas:\nUna capa inferior que mantiene la vista parcial y opera sobre la red no-estructurada. Una capa superior que genera una topología estructurada en base a la vista parcial. Rumores seguros # La velocidad de propagación de datos puede generar problemas de seguridad/confiabilidad.\nPor ejemplo, $c$ nodos pueden cooperar maliciosamente para cooptar la red:\nAl intercambiar las vistas parciales, estos nodos envian $c/2$ entradas que sólo referencian a alguno de estos $c$ nodos. Gradualmente, las vistas parciales de todos los nodos solo contienen referencias a un conjunto de estos $c$ nodos. Se busca tratar de detectar y prevenir comportamiento malicioso\nLos nodos maliciosos pueden ser detectados por el elevado número de referencias desde otros nodos (indegree)\nSin embargo, al detectarlos ya puede ser demasiado tarde\nUna manera de mitigar es requerir que los nodos generen estadísticas:\nAl intercambiar vistas parciales se puede realizar también estadísticas Es importante que no se sepa cuando se utilizan para actualizar la vista parcial o para generar estadísticas Un nodo malicioso no puede devolver siempre enlaces a otros nodos maliciosos, sería rápidamente descubierto No queda otra que, de vez en cuando, \u0026ldquo;jugar con las reglas\u0026rdquo; de los nodos benignos "},{"id":11,"href":"/docs/06-nombres-intro/","title":"Nombres, identificadores y direcciones","section":"Docs","content":" Nombres, identificadores y direcciones # En el contexto de sistemas distribuidos, vamos a decir que un nombre es un conjunto de bits.\nEntidades # El principal uso de un nombre es permitir identificar una entidad.\nUna entidad puede ser cualquier cosa: una página web, una impresora, un proceso.\nEn general, una entidad se puede operar.\nPor ejemplo, si la entidad es una impresora, se puede enviar un documento para su impresión. Puntos de acceso # Para operar una entidad se requiere acceder a la misma mediante un punto de acceso.\nUn punto de acceso es una entidad\u0026hellip; y requiere un nombre. Una entidad puede tener uno, dos o más puntos de acceso. El punto de acceso puede cambiar con el tiempo. Ejemplo: 200.45.21.42:80 Dirección # Una dirección es un nombre que identifica la ubicación de una entidad.\nEl nombre de un punto de acceso es una dirección.\nEs mucho más flexible mantener el nombre de una entidad independiente de su dirección.\nLa entidad puede moverse de ubicación, con lo cual cambia su dirección\nSe conoce como independencia de ubicación\nIdentificadores # Los nombres que refieren univocamente a una entidad se conocen como identificadores\nTodo identificador que se precie cumple con lo siguiente:\nRefiere a una única entidad. Dicha entidad sólo es referenciado por ese identificador. Nunca es reutilizado. Muchas veces un nombre o un identificador esta pensando para que lo lea una computadora.\nPor ejemplo, una cadena ilegible como una dirección MAC: 00:26:c7:d9:98:54 Pero otros nombres son diseñados para que sean fácilmente legibles por un humano.\nEjemplo: nombres de archivo, DNS Resolución de nombres # Tiene que existir un mecanismo que resuelva los nombres.\nEsto es, obtener información acerca de la entidad en base a su nombre.\n¿Cómo resolvemos un nombre a una entidad? Basicamente, hay dos opciones:\nMantener un registro de tipo (nombre, dirección)\nRealizar un ruteo hacia la dirección o punto de acceso\nGeneralmente utilizado en redes p2p. "},{"id":12,"href":"/docs/06-nombres-planos/","title":"Nombres planos","section":"Docs","content":" Nombres planos # No contienen información acerca de la entidad, su ubicación o su punto de acceso.\nEn general los identificadores son de este tipo.\nEjemplo:\n0b0adad386f3f0836c994e8487c1b470cbb6f682 (suerte con intentar conocer la entidad sin contexto)\n00:26:c7:d9:98:54\nSon sencillos de generar, pero transfieren la complejidad al mecanismo de resolución de nombres.\n¿Cómo se puede resolver la entidad asociada?\nVamos a ver una serie de posibles soluciones simples:\nBroadcast Multicast Forwarding Pointers Basados en hogar Broadcast # Una opción es realizar un broadcast del identificador.\nUna red LAN (cableada o wireless) ofrece servicios eficientes de broadcast.\nCada nodo chequea si contiene la entidad asociada al identificador recibido.\nEjemplo: ARP\nProblema: a medida que la red incrementa su tamaño, el uso de broadcast se vuelve ineficiente.\nMulticast # El objetivo es evitar interrumpir nodos que no esten interesados en el mensaje.\nPosible de implementar en redes Ethernet\nEn IP se puede definir grupos de multicasting\nCada grupo es identificado mediante una dirección. Útil como mecanismo de ubicación\nEl multicast puede ser una consulta por la dirección de un nodo en particular. Otro uso es enviar una petición a múltiples replicas.\nForwarding Pointers # Mantener una referencia a la nueva ubicación una entidad (dio para una tesis)\nSu ventaja es la sencillez: basta seguir la cadena de referencias para ubicar la entidad.\nEjemplo # Si una entidad se mueve de A a B, entonces en A queda una referencia a B.\n¿Y si luego se mueve a C? Entonces $A \\rightarrow B \\rightarrow C$\n¿Y si despues se mueve a D? Creo que se capta la idea\u0026hellip;\nDesventajas # La cadena de referencias puede terminar siendo demasiado extensa.\nLas ubicaciones intermedias deben preservar las referencias.\nLa cadena es vulnerable a la pérdida de alguno de sus componentes.\nPor lo tanto, es una solución principalmente aplicable en LANs.\nBasados en hogar (home-based) # Consiste en mantener una referencia a la ubicación actual de una entidad.\nLa referencia se mantiene en una entidad conocida como hogar (home location)\nPor lo general, el hogar es donde se creo la entidad inicialmente.\nDesventajas # Incremento de la latencia.\nEl hogar siempre tiene que existir.\nAplicaciones # Utilizado para referir entidades móviles en redes de gran escala\nSirve como mecanismo de respaldo para servicios basados en forwarding pointers\nEjemplo: Mobile IP # Mobile IP\nRFC 5944 - IP Mobility Support for IPv4, Revised RFC 6275 - Mobility Support in IPv6 Es un estándar de la IETF que permite a un dispositivo móvil mantener una dirección IP permanente.\nIntenta ofrecer un elevado nivel de transparencia de ubicación.\nFuncionamiento:\nCada nodo móvil tiene una IP fija.\nLa comunicación inicial con el nodo móvil se realiza mediante el home agent (vendría ser el hogar)\nEl home agent reside en la red origen, por lo general donde se generó el nodo móvil Cuando el nodo se muda a otra red, solicita allí una nueva IP que registra en el home agent\nEsta dirección se conoce como care-of address Cuando el home agent recibe una consulta para el nodo, se la reenvía.\nPuede usar por ejemplo, tunneling Al mismo tiempo, el emisor de la consulta recibe del home agent la ubicación actual del nodo móvil.\nEl nodo móvil, luego se comunica directamente con el emisor de la consulta.\nEste proceso se oculta en lo posible a la aplicación.\n"},{"id":13,"href":"/docs/06-nombres-planos-chord/","title":"Chord","section":"Docs","content":" Chord # Chord es un sistema DHT (Distributed Hash Table) relativamente sencillo de entender, utilizado en redes p2p\nSistemas similares:\nPastry Tapestry Material de lectura # Estos apuntes estan basados en:\nLa sección 6.2.3 Distributed Hash tables del libro Distributed Systems, donde se describe brevemente Chord. No es necesario leer la nota avanzada, pero sí la subsección Exploiting network proximity.\nEn el paper original. Recomendado leer la sección 1 y la sección 4.3 (hasta el Teorema, que no es necesario leer ni entender).\nSimulador # Pueden probar como funciona Chord con este simulador\n¿Pero qué es? # Es un sistema de lookup distribuido.\nEn criollo: dado una clave (nombre) obtiene el nodo asociado.\nEj: lookup(key) retorna la IP del nodo asociado.\nY lo trata de hacer de manera eficiente\nEl protocolo especifica:\nComo realizar el lookup (obviamente).\nComo nuevos nodos se unen al sistema.\nComo manejar la salida de nodos (planeada o no).\nPropiedades bonitas # Balanceo de carga\nDescentralizado\nEscalable\nAlta disponibilidad\nFlexibilidad en asignación de nombres\n(Estas propiedades son las que debe tener cualquier servicio de nombres)\nDescripción general # Usa un espacio de $m$ bits para asignar identificadores a nodos y claves a entidades.\nEl número $m$ de bits usualmente esta entre 128 y 160.\nTiene que ser lo suficientemente grande para evitar colisiones. Requiere hash consistente para distribuir de manera uniforme las claves en el espacio de nombres.\nEj: SHA1 El objetivo es todos los nodos administren (aproximadamente) la misma cantidad de claves. Una entidad con clave $k$ es administrada por el nodo con el menor identificador $p$ tal que $p \\geq k$.\nPara simplificar, vamos a decir que la entidad $k$ es administrada por el nodo $p$, tal que $p \\geq k$ Dicho nodo se denomina sucesor de $k$ y se denota $succ(k)$\nLos nodos se organizan en un anillo\nCada nodo mantiene referencias a su sucesor y predecesor Resolución de nombres # Problema: Dado un clave $k$ ¿Cómo resolver la dirección de $succ(k)$?\nSolución lineal # Veamos primero una solución que sencilla, que no usa Chord.\nLo más simple es pasar la consulta al siguiente nodo en el anillo.\nCuando un nodo $p$ recibe una consulta por una entidad $k$\nResponde la consulta si $pred(p) \u0026lt; k \\leq p$ Caso contrario, pasa la consulta a $succ(p+1)$ En promedio, una consulta requerirá recorrer la mitad del anillo.\nSolución exponencial # Chord intenta resolver eficientemente la dirección de $succ(k)$\nCada nodo mantiene una tabla con $s \\leq m$ entradas conocida como tabla finger\nLa tabla del nodo $p$ se denota $F_p$\nCada entrada $i$ contiene lo siguiente:\n$$ F_p[i] = succ((p + 2^{i-1}) \\quad mod \\quad 2^m) $$\nPor ejemplo, con $m=5$ ($2^5=32$ identificadores), la tabla del nodo 1 ($p=1$) es:\níndice sucesor 1 $ succ(1 + 2^{1-1}) = 2 $ 2 $ succ(1 + 2^{2-1}) = 3 $ 3 $ succ(1 + 2^{3-1}) = 5 $ 4 $ succ(1 + 2^{4-1}) = 9 $ 5 $ succ(1 + 2^{5-1}) = 17 $ Cada entrada contiene el identificador del primer sucesor a una distancia de al menos $2^{i-1}$ unidades.\nCada nodo sólo guarda una cantidad reducida de información de los demás.\nNotar que $F_p[1]$ es el sucesor del nodo en el anillo.\nAhora, cuando un nodo $p$ recibe una consulta por una entidad con clave $k$:\nResponde la consulta si $k \\in (pred(p), p]$ Si $p \u0026lt; k \\leq F_p[1]$ reenvia la consulta a su sucesor Caso contrario, reenvia la consulta al nodo $i$ tal que $F_p[i] \\leq k \u0026lt; F_p[i+1]$ Osea, $k \\in (,]$ El costo en general sera $O(log(N))$\nEjemplo # Notar que no tienen que existir $2^m$ nodos: pueden haber muchos menos en el anillo.\nPor ejemplo, para $F_p(1)$, $succ(2) = 4$ ya que el nodo 4 es responsable de las claves 2, 3 y 4.\nTopología dinámica # Los nodos pueden entrar o salir del anillo, voluntariamente o por un fallo\nUn nodo $p$ que quiera sumarse a una anillo Chord debe:\nConsulta $succ(p+1)$ a alguno de los nodos existentes Luego se agrega como predcesor de $succ(p+1)$ Se tienen que actualizar las tablas finger\nPeriódicamente mediante algun hilo o proceso en segundo plano Para la primer entrada ($F_p[1]$, o sea el sucesor):\nPeriodicamente consulta si sigue siendo el predecesor de su sucesor. Osea, el nodo $p$ verifica si $p=pred(succ(p+1))$ Si no lo es, entonces se sumo un nuevo nodo $q$ Repite el proceso con $q$ como sucesor. Para el resto de las entradas, debe encontrar el $succ(p + 2^{i-1})$\nDe manera similar, cada nodo debe verificar su predecesor:\nSi un nodo $q$ detecta que su predecesor no esta presente, lo marca como desconocido Si el nodo $q$ detecta que su sucesor no conoce a su predecesor, se presenta como tal De la misma manera, un nuevo predecesor se presentará como tal a $q$ Proximidad # Un problema potencial es que dos nodos lógicamente próximos pueden estar físicamente lejanos\nNodo 19 en Madryn, nodo 20 en Tokio, nodo 21 en Madrid Posibles soluciones:\nAsignar los identificadores a nodos en base a la topología\nNodos físicamente cercanos tendrán identificadores próximos Ruteo por proximidad\nSe mantienen varios sucesores por cada entrada de la tabla finger Selección del vecino más cercano\nVálida cuando puede haber varios nodos que sirvan de predecesor. No es el caso de Chord, pero sí de Pastry. "},{"id":14,"href":"/docs/07-replicacion/07-01-introduccion/","title":"Consistencia","section":"Docs","content":" Introducción # Razones para replicar # Existen dos razones para replicar:\nIncrementar la confiabilidad del sistema Mejorar la perfomance Tamaño: incrementar el número de peticiones que se atienden Área geográfica: reducción de lantecias (acercar copias a los clientes) Todo muy bonito ¿Quién podría estar en contra de replicar?\nLa cruda realidad: hay un precio que pagar, ya que replicar genera problemas de consistencia.\nCada vez que se modifica una replica, se tienen que actualizar el resto de las copias.\nCuándo y cómo se realizan estas actualizaciones determina el costo de la replicación.\nEscalabilidad mediante replicación # En general, los problemas de escalabilidad aparecen como problemas de perfomance.\nSe intentan paliar con una combinación de replicación y cacheo\nLa replicación es iniciada por el servidor y el cacheo por el cliente. Ejemplo:\nProblema: alta latencia en las peticiones Solucion: agregar replicas cercanas a los clientes Nuevo problema: ancho de banda requerido para mantener todas las replicas consistentes. Otro ejemplo:\nSuponer un cliente que hace requerimientos a un ritmo $N$. La replica más cercana al cliente se actualiza a un ritmo $M$. Si $N \u0026laquo; M$, entonces se esta desperdiciando ancho de banda. Ademas, mantener las replicas sincronizadas\u0026hellip; atenta contra la escalabilidad\n¡Justamente lo que estamos intentando mejorar!\nPor ejemplo, una transacción atómica sobre múltiples replicas.\nMantener las replicas sincronizadas requiere coordinación lo que puede ser muy costoso.\nDilema: aunque los problemas de escalabilidad pueden ser resueltos con replicación, mantener las copias sincronizadas es costoso. ¿El remedio es peor que le enfermedad?\nSolución\nUna posible solución es relajar los requerimientos de consistencia\nLas actualizaciones no requieren ser operaciones atómicas\nLas replicas entonces pueden no estar sincronizadas\nCúanto y cómo disminuir la consistencia depende del patrón lectura/escritura.\n"},{"id":15,"href":"/docs/07-replicacion/07-02-data-centric/","title":"Consistencia centrada en datos","section":"Docs","content":" Modelos de consistencia centrado en datos # Generalidades # Suponer un data store replicado en varias máquinas (nodos).\nUn modelo de consistencia es un contrato entre los clientes y el data store\nSi se respeta el contrato, todos contentos Por ejemplo, normalmente un cliente espera que una lectura refleje la última escritura.\n¿Pero qué es la última escritura en un sistema distribuido?\n¡No tenemos un reloj global! Diferentes modelos de consistencia dan una respuesta distinta:\nEn un extremo, los modelos \u0026ldquo;relajados\u0026rdquo;:\nMejor desempeño Más complicados de implementar / entender. En el otro, los modelos \u0026ldquo;estrictos\u0026rdquo;:\nMenor desempeño. Más fáciles de entender / utilizar. No existe el modelo de consistencia ideal\nOrden consistente de las operaciones # Consistencia secuencial # Cualquier orden de operaciones de lectura / escritura es válido.\nTodos los nodos deben ver el mismo orden de operaciones.\nDebe respetar el orden de operaciones en cada programa.\nPor lo tanto, los clientes deben aceptar cualquier orden (válido) de lectura.\nEjemplos:\nZookeeper Zookeeper logra la consistencia secuencial mediante su protocolo ZAB Mas info:\nJepsen Consistencia causal # Tiene en cuenta las operaciones que pueden estar causalmente relacionadas.\nEntonces, si $a \\rightarrow b$ todos los procesos deben ver primero $a$ y luego $b$.\nLas operaciones concurrentes pueden aparecer en cualquier orden.\nRequiere mantener información de qué proceso vio cuál actualización.\nEjemplo:\nDynamoDB Mas info:\nJepsen Agrupamiento de operaciones # Uso de secciones críticas (ingreso y salida).\nAl ejecutar un ingreso, el proceso sabe que todos los datos de su data store estan actualizados.\nEl uso de estos locks debe respetar la consistencia secuencial\nConsistencia eventual # Se basa en la siguiente observación:\nLa mayoría de los procesos realizan lecturas Sólo unos pocos nodos realizan escrituras ¿Que tán rápido deberían propagarse estos cambios?\nSi lo clientes se conectan siempre a la misma replica\u0026hellip; en general no verían inconsistencias.\nPodemos propagar los cambios de manera \u0026ldquo;lenta\u0026rdquo; (ideal para WANs) Un ejemplo son las páginas web.\nDado un lapso de tiempo eventualmente todas las replicas tendrán el mismo valor.\nSuponiendo que no existen conflictos write / write\nRequiere que se garantice que las actualizaciones serán enviadas a todas las replicas.\nBueno, suponer que no hay conflictos es algo ideal:\nProblemas write/write:\nSon pocos los nodos que escriben, se pueden solucionar fácilmente Por ejemplo una de las operaciones write sobreescribe al resto. Uso de CRDT (conflict-free data types) Permiten actualización sin necesidad de coordinación Consistencia eventual fuerte: aún cuando haya conflictos, las replicas donde se aplicaron tendrán el mismo estado. Si todo falla, utilizar algun mecanismo de exclusión mutua. Ejemplos:\nCosmosDB Cassandra Consistencia continua # Muchas aplicaciones pueden tolerar falta de consistencia.\nPero requieren también definir algun tope o cota a dicha inconsistencia.\nEn general, especifican una desviación tolerable como un rango sobre un atributo:\nDesviación numérica Absoluta Relativa Antigüedad Orden de los eventos Número de actualizaciones Conit # Unidad de consistencia (consistency unit)\nEspecifica la unidad sobre la cual la consistencia va a ser medida.\nEjemplo, una acción en un mercado de valores. Material de lectura # Libro Distributed Systems 4th # Leer las siguientes secciones:\n7.2.1: consistencia secuencial y causal, agrupamiento de operaciones. No es necesario leer las notas 7.1 y 7.2 (aunque son interesantes). 7.2.2: consistencia eventual (no es necesario leer la nota 7.3) 7.2.3: consistencia continua (no es necesario leer las notas 7.4 y 7.5) "},{"id":16,"href":"/docs/07-replicacion/07-03-client-centric/","title":"Consistencia centrada en el cliente","section":"Docs","content":" Modelos de consistencia centrados en el cliente # Considerar un data store distribuido donde:\nno existen actualizaciones simultaneas o bien pueden ser fácilmente resueltas. la mayoría de las operaciones son de lectura. modelos de consistencia débil, por ejemplo eventual Un data store así funciona bien mientras un cliente se conecte siempre a la misma replica.\nDenominado sticky-availability (¿disponibilidad pegajosa?)\nLos problemas aparecen si se conectan otra replica en un lapso corto de tiempo\nMas precisamente, en un lapso menor que el tiempo de propagación de las actualizaciones. ¿Cómo se pueden manejar las inconsistencias?\nSe han propuesto una serie de modelos, conocidos como \u0026ldquo;centrados en el cliente\u0026rdquo;\nSe denominan así por que ofrecen garantías para un único cliente. Son conocidas como consistencia de sesión (session consistency)\nProveen garantías dentro de una sesión iniciada por el cliente, que agrupa operaciones de lectura/escritura. Existen principalmente cuatro modelos:\nLecturas monotónicas Escrituras monotónicas Lee tus escrituras Escrituras siguen lecturas En las descripciones que siguen la operación de lectura y/o escritura son realizadas por el mismo proceso\nLecturas monotónicas # Al leer el valor de un item x, una lectura posterior de dicho item devuelve el mismo valor o uno más reciente.\nEn otras palabras, nunca se lee un valor más antiguo que el último leído.\nEjemplo: en un calendario online, siempre quiero leer la versión más reciente de un evento.\nEscrituras monotónicas # La operación de escritura en un item x es finalizada antes que cualquier otra operación de escritura en el mismo item x\nEn castellano, las operaciones de escritura son propagadas en el mismo orden a todas las réplicas.\nEjemplo: las modificaciones a un archivo deben siempre estar en el orden en que fueron realizadas.\nLee tus escrituras # Los efectos de una escritura en el item x serán siempre vistos por sucesivas lecturas de dicho item\nEn criollo, cualquier operación de escritura es finalizada antes que cualquier operación de lectura sin importar en qué replica se realice.\nEjemplo: no ver contenido eliminado al conectarse a una replica diferente.\nEscrituras siguen lecturas # Una escritura en el item x, posterior a una lectura de dicho item, modifica el valor previamente leído de x o uno más actualizado\nSencillamente, cualquier operación de escritura en x será realizada sobre una copia de x con el valor más recientemente leído por el proceso.\nAlso: no se puede cambiar el pasado (la operación de escritura no modifacará un valor anterior a la última lectura).\nEjemplo: no se leen respuestas a un correo electrónico sin tener el correo electrónico al que se responde.\nEjemplos # Consistencia en Zookeeper # Zookeeper garantiza que las actualizaciones son serializables y conservan la precedencia.\nEl estado es un ordenamiento lineal de todas las escrituras, respetando escrituras monotónicas.\nEn dicho estado, cada cliente ve reflejada sus operaciones en el orden en que las envío.\nZookeeper:\nrespeta escrituras y lecturas monotónicas. no garantiza lee tus escrituras ni escrituras siguen lecturas Cosmos DB # En CosmosDB la consistencia de sesión, dentro de una única sesión de cliente, garantiza que las lecturas respetan lee tus escrituras y escrituras siguen lecturas. Esta garantía asume una única sesión de escritura o compartir el token de la sesión entre múltiples escritores.\nMaterial de lectura # Requerido # La sección 7.3 Client-centric consistency models del libro Distributed Systems 4th describe los modelos anteriores.\nAdicional # Distintos modelos de consistencia explicados mediante un juego de béisbol (paper)\nDescripciones de los anteriores modelos, con algo de información adicional:\nLecturas monotónicas Escrituras monotónicas Lee tus escrituras Escrituras siguen lecturas Esta entrada de blog presenta un ejemplo de cuando no se garantiza la consistencia lee tus escrituras (un cliente lee datos incorrectos tras escribir) debido al retraso en la replicación. Incluye soluciones que se ven tratan en protocolos.\nEn esta entrada se tiene otra explicación del modelo lee tus escrituras con algunos detalle de su implementación.\n"},{"id":17,"href":"/docs/07-replicacion/07-04-replica-managment/","title":"Administración de réplicas","section":"Docs","content":" Administración de réplicas # ¿Dónde ubicar las réplicas? Dos aspectos:\nDistribución física de los servidores. Distribución del contenido en dichos servidores. ¿Qué mecanismos utilizar para mantener las replicas (eventualmente) consistentes?\nUbicación física # Hoy quiza no tan relevante dada la computación en la nube.\nSe resume en un problema de optimización.\nSeleccionar las mejores $K$ ubicaciones de $M$ posibles ($K \u0026lt; M$) Y por lo tanto, computacionalmente costoso: se aplican heurísticas Tipos de criterios para ubicar un servidor:\nRed (técnicos): Latencia, ancho de banda, hops, etc. Económicos (costos) Modelos para decidir donde ubicar una réplica:\nQoS: Optimizar uno o más parámetros de QoS Ofrecer alguna garantía, por ejemplo de ancho de banda Computacionalmente complejo, se aplican heurísticas Consistencia: Tiene en cuenta los costos de mantener actualizadas las replicas Subclases: Actualizaciones periódicas / aperiódicas Expiración Caches Basados en patrones de lectura/escritura Energía Ubicación en base al consumo / gasto de energía Por ejemplo, evitar servidores que esten ociosos También maximizar el trabajo realizado por unidad de energía Otros (politicas, costos de CDNs, etc) Ubicación del contenido # ¿En qué replicas ubicar un cierto contenido?\nSe pueden diferencias replicas:\nPermanentes # Conjunto inicial de servidores Número relativamente pequeño, posiblemente estático En general, útiles como backups Ejemplo: mirroring Iniciadas por el servidor # Objetivo: mejorar la perfomance. Generalmente con contenido sólo-lectura Ejemplo: CDN Iniciadas por el usuario # Más conocidas como caches Administradas por el cliente Pueden ser compartidas por varios clientes. Mejoran el tiempo de acceso Ejemplo: caches de navegadores web. Propagación # ¿Qué propagar?\nNotificaciones Datos Operaciones Notificaciones # A.K.A protocolos de invalidación\nLas replicas son notificadas que los datos ya no son actuales.\nLas replicas luego deben iniciar un mecanismo de actualización.\nRequiere poco ancho de banda.\nConveniente cuando existen más operaciones de actualización que de lectura.\nRelación Read-to-write pequeña. Datos # A.K.A replicación pasiva\nSe transfieren los datos modificados a las replicas.\nPuede transferirse todo o sólo las modificaciones (ejemplo, un diff).\nUna misma comunicación puede incluir múltiples actualizaciones.\nConveniente cuando el número de lecturas es mayor que las actualizaciones.\nRelación Read-to-Write alta. Operaciones # A.K.A replicación activa\nConsiste en propagar la operación de actualización, posiblemente con parámetros.\nReduce el ancho de banda requerido.\nSin embargo, puede incrementar el costo computacional en las réplicas.\nPull vs Push # Dilema: ¿envío o recibo?\nPush # A.K.A protocolos basados en el servidor\nLas actualizaciones son propagadas sin necesidad de que sean solicitadas.\nUtilizado generalmente con las replicas iniciadas por el servidor.\nÙtil cuando se requiere un modelo de consistencia fuerte.\nRatio Read-to-Update alto. Pull # A.K.A protocolos basados en el cliente\nEl cliente solicita el envío de las actualizaciones.\nMuy utilizado para el manejo de las caches.\nEficiente cuando el ratio Read-To-Update es bajo.\nLease # Es un enfoque híbrido.\nSe obtiene una \u0026ldquo;promesa\u0026rdquo; del servidor que enviará una actualización.\nBasados en:\nAntigüedad Frecuencia de actualización Estado del servidor Comparativa # Según estado del servidor:\nPush requiere conocer las replicas y su estado. Pull no requiere información de las replicas. Mensajes:\nPush requiere enviar mensajes de actualización (y quiza fetchs) Pull requiere mensajes de polling y fetchs/update. Tiempo de respuesta:\nInmediato (o lo que tarde el fetch-update) Lo que requiera el fetch-update. "},{"id":18,"href":"/docs/07-replicacion/07-05-protocolos/","title":"Protocolos de consistencia","section":"Docs","content":" Protocolos de consistencia # Los modelos de consistencia se implementan siguiendo algún protocolo.\nBasados en un primario # Utilizados para implementar modelos de consistencia secuencial.\nCada item $x$ tiene asociado un nodo (replica) primario en el data store.\nEste nodo es responsable de coordinar las operaciones de actualización.\nOsea, un unico nodo o replica es responsable de realizar las actualizaciones\nLuego las propaga al resto de los nodos, claro. El nodo primario puede estar\nfijo (osea, ser remoto) trasladarse (se traslada a donde se realice la escritura) Primario remoto # A.K.A primary-backup, escritura-remota\nCuando una réplica recibe una operacion de escritura, la reenvia al nodo primario.\nEl nodo primario coordina cómo y cuando relizar la escritura, y lo comunica al resto de las replicas.\nCuando todas las réplicas realizaron la operación, un ACK es envíado al cliente.\nRelativamente sencillo de implementar\nPosibles problemas de performance por la espera (bloqueante)\nVariante: informar el ACK cuando el primario recibe la operación, sin esperar la propagación (no-bloqueante)\nMediante este protocolo, es posible implementar modelos de consistencia secuencial\nAdemás, la versión bloqueante permite leer siempre la última escritura.\nPrimario local # A.K.A escritura-local\nEn este caso, el primario migra a donde sea requerida la operación de escritura.\nVentaja: permite realizar múltiples operaciones de escritura localmente.\nRequiere una propagación de la actualización de manera no-bloqueante.\nÚtil es computación móvil:\nel dispositivo puede volverse el primario de los datos que espera modificar mientras este desconectado. el resto de los nodos puede realizar operaciones de lectura (pero no escrituras) Primario local + Remoto # En un modelo de primario remoto, una replica puede actuar como primario local momentaneamente.\nDe esta manera múltiples operaciones de escritura tienen una mejor performance.\nLa replica luego informa al primario remoto las modificaciones para su propagación.\nEjemplo: sistemas de archivos distribuidos.\nEscrituras replicadas # En este tipo de protocolos, múltiples replicas pueden realizar operaciones de escritura.\nVariantes:\nReplicación activa (la operación se envía a todos las replicas) Quorum (basados en voto) Replicación activa # Cada replica tiene asociado un proceso que realiza actualizaciones.\nEn general se propaga la operación.\nPara consistencia, requiere:\nmulticas totalmente ordenado O bien, un secuenciador central que de un número secuencial a cada operación. Quorum # Las operaciones de lectura o escritura deben tener el permiso de la mayoría ($N/2 + 1$)\nDos tipos de quorum:\nDe lectura ($Q_r$) De escritura ($Q_w$) Suponer que existen $N$ resplicas, entonces:\nSi $Q_r + Q_w \u0026gt; N$ no existen conflictos lectura-escritura (write/read)\nSi $Q_w \u0026gt; N/2$ no existen conflictos de escritura-escritura (write/write)\nSuponer 6 replicas. Entonces $Q_w \\geq 4$ y $Q_r \\geq 3$:\nCualquier quorum de lectura siempre detecta al menos una versión más reciente de un item.\nCualquier quorum de escritura evita un conflicto write/write\nROWA: Read-once, Write-all\nCaso especial si $Q_r = 1$ entonces $Q_w = N$ Mejora la performance de lectura, a costa de la de escritura. Coherencia de cache # Mantener la cache coherente con el estado de los servidores.\nEn el caso de sistemas distribuidos, la detección se realiza en tiempo de ejecución.\n¿Cómo forzar la coherencia?\nEnviar invalidaciones cuando los datos son replicados.\nPropagar la actualización\nNo permitir almacenar en cache datos compartidos entre clientes.\n¿Qué sucede cuando se modifican datos en la cache?\nwrite-through: envíar actualización a las réplicas\nwrite-back: envíar múltiples actualizaicones a las réplicas\nConsistencia centrada en el cliente # Sencilla de implementar si se ignora la performance.\nCada operacion de escritura $W$ tiene asignado un identificador global. Cada cliente tiene asociado dos conjuntos de escrituras:\nread-set: escrituras relevantes a las operaciones de lectura. write-set: el conjunto de escrituras del cliente. Lecturas monotónicas # El cliente pasa a la réplica la operación de lectura y el conjunto read-set.\nSi la replica no tiene las operaciones en el conjunto read-set:\nPide las operaciones a otro servidor que sí las tenga. O bien, deriva la operación de lectura. Escrituras monotónicas # El cliente pasa a la réplica la operación de escritura y el conjunto write-set.\nLa réplica debe verificar que cuenta con todas las operaciones de escritura.\nCaso contrario, o se actualiza o deriva la operación de escritura.\nCuando la operación de escritura se realiza con éxito, la misma es agregada en el write-set.\nLee tus escrituras # La replica debe contener todas las operaciones del conjunto write-set\nEscrituras siguen lecturas # La replica debe contener todas las operaciones del conjunto read-set\nUna vez realizada la escritura, esta se agrega tanto en write-set como en read-set\n"},{"id":19,"href":"/docs/08-tolerancia/08-01-introduccion/","title":"Tolerancia a fallos","section":"Docs","content":" Introducción a la tolerancia a fallos # Las fallas parciales son una característica de los sistemas distribuidos.\n¿Como diseñar los sistemas distribuidos para que se recuperen de este tipo de fallas?\nIdealmente automáticamente y sin degradar la performance.\nTolerancia a fallas # Relacionado con el concepto de dependable systems\nAlgunos terminos:\nDisponibilidad # Es la probabilidad que el sistema este funcionando en cualquier momento.\nConfiabilidad # La probabilidad que el sistema ejecute continuamente sin fallos.\nSeguridad # No ocurre una catástrofe cuando eventualmente ocurre una falla.\nMantenibilidad # Qué tan fácil es reparar el sistema ante una falla.\nMétricas # MTTF: Mean Time To Failure.\nMTTR: Mean Time To Repair.\nMTBF: Mean Time Between Failures ($MTTF + MTTR$)\nAvería (fail) # Cuando el sistema no cumple con sus especificaciones.\nError # La parte del estado de un sistema que puede producir una falla.\nFalla (Fault) # La causa de un error.\nUn sistema tolerante a fallos es aquel que puede proveer sus servicios aún ante la existencia de fallas.\nLas fallas se pueden clasificar en:\nTransitorias: ocurren una única vez. Intermitentes: se repiten frecuentemente. Permanentes: presente hasta que el componente afectado es reparado/reemplazado. Modos de falla # Clasificación:\nFalla de detención (crash failure)\nFalla de omisión\nRecepción Envío Falla temporal\nFalla de respuesta\nValor Transición de estado. Fallas arbitrarias\nOmisión Comisión Sincrónico / Asicrónico # Los modelos anteriores suponene que un proceso $P$ puede detectar que $Q$ se detuvo.\n¿Cómo es posible esto?\nHay que diferenciar tipos de sistemas:\nAsincrónico: no se puede suponer nada acerca de los tiempos de ejecución o de transferencia de mensajes. Sincrónico: los tiempos de ejecución y de transmisión estan acotados. Ninguno es un modelo realista.\nEn la práctica se asumen sistemas parcialmente sincrónicos:\nLa mayor parte del tiempo se comportan como sistemas sincrónicos. El comportamiento asincrónico es la excepción. Se pueden uilizar timeouts, con el riesgo de tener falsos positivos. En este contexto, las fallas de detención se pueden subclasificar:\nFail-stop: fallas de detención que pueden ser detectadas con seguridad. Fail-noisy: similar, pero un proceso eventualmente detecta la falla. Fail-silent: no se puede distinguir fallas de detención de fallas de omisión. Fail-safe: fallas arbitrarias benignas. Fail-arbitrary: no observables y dañinas, la peor combinación. Enmascaramiento de fallas # La mejor manera de tolerar una falla es ocultarla mediante redundancia.\nTipos de redundancia:\nInformación Temporal Física Un ejemplo es la redundancia modular triple.\n"},{"id":20,"href":"/docs/08-tolerancia/08-03-comunicacion/","title":"Comunicación confiable","section":"Docs","content":" Comunicación confiable # Los enlaces de comunicación también fallan.\nLos canales de comunicación pueden experimentar:\nFallas de detención Fallas de omisión (ej: mensajes perdidos) Temporales (ej: imeouts) Fallas arbitrarias (ej: mensajes duplicados) Comunicación punto a punto # La comunicación punto a punto puede ser resuelta mediante TCP.\nEnmascara fallas de omisión No resuelve fallas de detención. Semántica de RPC en caso de fallos # Clases de falla en RPC:\nEl cliente no puede ubicar el servidor.\nNo se puede ubicar el servidor. El stub del cliente es incompatible con una nueva versión. Posible solución: generar una excepción. La petición del cliente se pierde.\nSe reenvía si no se recibe respuesta o confirmación. Sencillo de implementar ¿Cómo identificar que se perdió? El servidor falla luego de recibir la petición.\nDos casos: Servidor falla antes de procesar la petición. Servidor falla luego de procesar la petición. Se deben tratar de manera separada. at-least-once: seguir insistiendo hasta que se recibe una respuesta. at-most-once: reportar el error inmediatamente. exactly-once: imposible No se puede garantizar que el servidor reciba la petición exactamente una única vez. Se puede \u0026ldquo;emular\u0026rdquo;, ej: recepción at-least-once combinado con peticiones idempotentes o control de duplicados. no-warranties: no garantizar ninguna de las anteriores. Moraleja: la recuperación transparente de un servidor no es posible. Se pierde la respuesta a la petición.\nTimeouts: no distingue entre mensajes perdidos y fallas temporales. Peticiones idempotentes: se pueden reenviar sin efectos secundarios. Números de secuencia: requiere mantener estado de los clientes. Información de retransmisión: bit o bandera que indique que es una retransmisión. El cliente falla luego de enviar la petición.\nGenera procesos \u0026ldquo;huerfanos\u0026rdquo; del lado del servidor. Consumen recursos. Pueden mantener recursos compartidos bloqueados. Pueden responder consultas más recientes, generando confusión. Soluciones: Exterminio: requiere log persistente de las operaciones, para identificar los procesos huerfanos y aniquilarlos. Reencarnación: similar, pero el cliente anuncia una nueva epoca (epoch), y se elimina a los huerfanos de épocas anteriores. Expiración: los procesos generados por una RPC deben completar antes de un tiempo $T$. Pueden recibir tiempo adicional del cliente. Si el cliente falla, pasado un tiempo $T$ los procesos huerfanos finalizan. No hacer nada: el cliente debe poder manejar respuestas de RPCs anteriores a su falla. Comunicación grupal confiable # Servicios que permitan que los mensajes sean entregados a todos los procesos dentro de un grupo.\nDistinción\nRecepción: el mensaje es entregado al middleware Entrega: el mensaje es entregado a la aplicación En casos de procesos que fallan, la comunicación es confiable cuando los mensajes son recepcionados y luego entregados a todos los participantes no defectuosos de un grupo.\nParte dificil: acordar cuales son los participantes del grupo.\nImplementación:\nComunicación punto-a-punto:\nSi el grupo es pequeño y los participantes conocidos. Ejemplo: conexión TCP para envio de mensaje y ACK Comunicación multicast\nEn general no confiable (mensajes perdidos) Uso de número de secuencia ACK de cada recepción O bien, incluido en otro mensaje (piggyback) Permite solicitar reenvio de mensajes perdidos El reenvio puede ser punto-a-punto Principal problema: escalabilidad\nEl emisor puede sufrir feedback implosion\nOsea, ser ahogado por ACKs o peticiones de reenvio Alternativa: sólo reenviar NACKs.\nMejora la escalabilidad. No garantiza la ausencia de feedback implosion ¿Cuanto tiempo guardar un mensaje? Clave: reducir el número de mensajes de feedback.\nScalable Reliable Multicasting (SRM)\nNo-jerarquico Se reportan únicamente NACKs. El NACKs se reporta mediante multicast Se realiza luego de un tiempo aleatorio Otro nodo que no haya recibido el mensaje puede así suprimir su NACK. Pero interrumpe nodos que sí recibieron el mensaje. Pueden asistir reenviando el mensaje. Jerárquico:\nÁrbol de difusión Cada nodo es un grupo de procesos Dentro del grupo se usa multicasting Existe un coordinador. Los coordinadores se comunican mediante enlaces punto-a-punto La difusión de un mensaje en cada grupo llega al coordinador El coordinador reenvía el mensaje a otros coordinadores. Un ACK o NACK de un coordinador representa a todo un grupo. Ambos tipos pueden ser combinados\n¿Rumores? Es una buena opción La difusión es más lenta, pero es escalable y robusta Multicast atómico # Garantizar que un mensaje sea entregado a todos los procesos no defectuoso o a ninguno.\nUn mensaje $m$ es asociado a una lista de procesos a los cuales debe ser entregado.\nEsta lista se conoce como vista de grupo.\nCada uno de los procesos en dicho grupo debe tener la misma vista de grupo.\nOsea, todos deben coincidir en cuales son los integrantes del grupo. Cuando un proceso se suma o abandona la vista, se dice que ocurre un cambio de vista\nGeneralmente anunciado también mediante un mensaje. Multicast virtualmente sincrónico # Si ante la falla del emisor del multicast de un mensaje $m$, el mismo es entregado a todos o a ninguno en el grupo, se dice que que el multicast es virtualmente sincrónico.\nEn otras palabras, un multicast no puede extenderse sobre dos vistas de grupo.\nO dicho de otra manera, las vistas de grupo son efectivamente barreras entre los multicast.\nOrden de los mensajes # El orden de los mensajes en un multicast puede ser:\nSin orden FIFO Causal Total En FIFO se garantiza que los mensajes de un mismo emisor son recibidos en orden, pero cada proceso puede ver un intercalamiento diferente de mensajes de distintos emisores.\nEn causal se garantiza que todos los procesos vean mensajes causalmente relacionados en el mismo orden.\nEl orden total requiere que todos los procesos reciban los mensajes en el mismo orden (sin importar cual sea).\nFinalmente, multicast atómico es un multicast confiable, virtualmente sincrónico, totalmente ordenado.\n"},{"id":21,"href":"/docs/","title":"Docs","section":"","content":" Unidades # "},{"id":22,"href":"/posts/06-nombres-planos/","title":"Nombres planos","section":"Posts","content":"No contienen ningún tipo de información acerca de la entidad ni de su punto de acceso.\nEjemplo: una dirección representada como una cadena aleatoria de bits.\n¿Cómo se puede resolver la entidad asociada?\nBroadcast # Para resolver un nombre plano se realiza un broadcast del identificador.\nUna red LAN (cableada o wireless) ofrece servicios eficientes de broadcast.\nCada máquina chequea si contiene la entidad asociada al identificador.\nEjemplo: ARP\nA medida que la red incrementa su tamaño, el uso de broadcast se vuelve ineficiente.\nAlternativa: multicasting\nEvita interrumpir nodos que no esten interesados en el mensaje.\nPosible de implementar en redes Ethernet\nEn IP se puede definir grupos de multicasting, identificados mediante una dirección.\nÚtil como mecanismo de ubicación\nSe puede utilizar para enviar la petición a múltiples replicas.\nForwarding Pointers # Mantener una referencia a la nueva ubicación una entidad.\nEjemplo: si una entidad se mueve de A a B, entonces en A queda una referencia a B.\nEs sencillo, basta seguir la cadena de referencias para ubicar la entidad.\nDesventajas:\nLa cadena de referencias puede terminar siendo demasiado extensa.\nLas ubicaciones intermedias deben preservar la referencia.\nLa cadena de referencias es vulnerable a la pérdida de alguno de sus componentes.\nBasados en hogar (home-based) # Consiste en mantener una referencia a la ubicación actual de una entidad.\nLa referencia se mantiene en una entidad conocida como home location\nUtilizado para referir entidades móbiles en redes de gran escala\nMecanismo de respaldo para servicios basados en forwarding pointers\nEjemplo: Mobile IP\nOfrece un elevado nivel de trasnparencia de ubicación.\nFuncionamiento:\nCada nodo móvil tiene una IP fija.\nLa comunicación inicial con el nodo móvil se realiza con el home agent\nEl home agent reside en la red inicial\nCuando el nodo se mueve a otra red, solicita una nueva IP que es registrada en el home agent\nCuando el home agent recibe un request para el nodo, reenvia el paquete al nodo.\nEl emisor es informado por el home agent de la ubicación del nodo móvil.\nEste proceso se oculta en lo posible a la aplicación.\nDesventajas:\nIncremento de latencia.\nLa ubicación hogar siempre tiene que existir.\n"},{"id":23,"href":"/posts/06-nombres-planos-chord/","title":"Nombres planos - DHT","section":"Posts","content":"Chord es un sistema DHT (Distributed Hash Table) relativamente sencillo de entender.\nMecanismo general # Usa un espacio de $m$ bits para asignar identificadores a nodos y claves a entidades.\nEl número $m$ de bits usualmente esta entre 128 y 160.\nUna entidad con clave $k$ es administrada por el nodo cuyo identificador $id$ sea $id \\geq k$.\nA dicho nodo se le denomina sucesor de $k$ y se denota como $succ(k)$\nProblema: ¿Cómo resolver eficientemente $k$ a la dirección de $succ(k)$?\nSolución lineal # xxx\nTablas finger # yyy\n"},{"id":24,"href":"/posts/06-nombres-intro/","title":"Nombres, identificadores y direcciones","section":"Posts","content":"Un nombre es un conjunto de bits que permiten identificar una entidad.\nUna entidad puede ser cualquier cosa: una página web, una impresora, un proceso.\nEn general, una entidad se puede operar.\nPor ejemplo, si la entidad es una impresora, se puede enviar un documento para su impresión. Para operar una entidad se requiere acceder a la misma, mediante un punto de acceso.\nUn punto de acceso es una entidad en un sistema distribuido\u0026hellip; y requiere un nombre. Una entidad puede tener uno, dos o más puntos de acceso. El punto de acceso puede cambiar con el tiempo. Ejemplo: IP:PUERTO El nombre de un punto de acceso se conoce como dirección.\nEs mucho más flexible mantener el nombre de una entidad independiente de su dirección.\nSe conoce como independencia de ubicación Los nombres que refieren univocamente a una entidad se conocen como identificadores\nRefiere a una única entidad. Cada entidad es referida solamente por un identificador. Los identificadores no se reutilizan. En general, nombres e identificadores son representados machine-readable.\nOtros nombres son diseñados para que sean fácilmente legibles por un humano.\nEjemplo: nombres de archivo, DNS ¿Cómo resolvemos un nombre a una entidad? Dos opciones:\nMantener un registro (nombre, dirección) Realizar un ruteo hacia la dirección o punto de acceso "},{"id":25,"href":"/posts/05-coordinacion/","title":"Coordinación","section":"Posts","content":"En un sistema centralizado x = timestamp(); y = timestamp() da como resultado $x \\leq y$.\nEn un sistema distribuido, acordar en un valor temporal no es trivial.\n¿Es posible sincronizar los relojes de los nodos de un sistema distribuido? La respuesta es sorprendentemente complicada.\nEntonces\u0026hellip; ¿Cómo coordinan sus actividades los procesos de un sistema distribuido?\nRelojes físicos # Existen situaciones donde es necesario que todos los nodos en un sistema acuerden en un valor de tiempo determinado.\nUn reloj físico presenta deriva\nProblemas:\n¿Cómo sincronizamos el reloj interno con un reloj externo? ¿Cómo sincronizamos los relojes internos entre sí? UTC:\nCoordinated Universal Time Estándar internacional 40 emisoras de onda corta difunden una señal al comienzo de cada segundo UTC Precisión $\\pm 1$ ms a $\\pm 10$ ms Uso de satelites, por ejemplo GPS y relojes atómicos Precisión $\\pm 0,5$ ms Sincronización de relojes:\nSuponer un conjunto de nodos: Desafio 1: que esten sincronizados con una referencia externa, por ejemplo UTC. Desafio 2: que los relojes de los nodos difieran lo menos posible. Exactitud: mantener la desviación con respecto a una fuente externa dentro de un rango específico. Precisión: mantener la desviación entre dos relojes dentro de un rango específico. Sincronización externa: mantener los relojes exactos. Sincronización interna: mantener los relojes precisos. Problema: Los relojes fisicos no son exactos, presentan deriva El reloj por software se basa en el reloj hardware Segun la deriva un reloj puede ser más rapido o más lento en referencia a un reloj ideal Fun facts: Dos relojes exactos pueden ser precisos con una cota $2 \\delta$ Sin embargo, ser precisos no indica nada acerca de la exactitud. Servidor de tiempo:\nObtener un valor actualizado desde un servidor central (por ejemplo, que tenga un reloj UTC) Implementación mediante una arquitectura cliente/servidor: Servidor retorna al cliente respuesta con timestamp. Cliente debe compensar el offset y delay. NTP\nNetwork Time Protocol Ambientes inalambricos:\nRequieren algoritmos diferentes Ejemplo: Reference Broadcast Synchronization Relojes lógicos # Generalmente lo que importa es que los nodos esten de acuerdo en el orden de los eventos.\nRelación happened-before:\n$a$ y $b$ son eventos Si $a$ ocurre antes que $b$ en un mismo proceso, entonces $a \\rightarrow b$ Si $a$ es el envío de un mensaje y $b$ la recepción, entonces $a \\rightarrow b$ Transitivo: si $a \\rightarrow b$ y $b \\rightarrow c$, entonces $a \\rightarrow c$ Introduce un ordenamiento parcial sobre los eventos. Diseño:\nCada evento $e$ tiene asociado un timestamp $C(e)$ Propiedad 1: si $a$ y $b$ son eventos en un mismo proceso y $a \\rightarrow b$ entonces $C(a) \u0026lt; C(b)$ Propiedad 2: si $a$ y $b$ son envío y recepción de un mensaje respectivamente, entonces $C(a) \u0026lt; C(b)$ Implementación:\nCada proceso $P_i$ mantiene un reloj lógico $C_i$ Por cada evento en $P_i$, $C_i = C_i + 1$ Cada mensaje enviado por Pi tiene un timestamp ts(m) = C(i) Cuando Pj recibe un mensaje m: Ajusta su contador Cj con el máximo valor entre Cj y ts(m) Antes de pasar el mensaje a la aplicación, incrementa Cj en uno. El servicio de relojes lógicos es implementado en un middleware, idealmente la aplicación no tiene por que ocuparse.\nEjemplo de uso: multicast totalmente ordenado\nRelojes vectoriales # Al usar relojes lógicos si $C(a) \u0026lt; C(b)$ no implica que $a$ ocurra antes que $b$\nSolución: relojes vectoriales\nCada nodo mantiene su propio reloj lógico Mantiene información de los relojes lógicos del resto de los nodos Se organizan como un vector o arreglo $V[i]$ es el reloj lógico del nodo $i$ $V[j]$ es el reloj lógico del nodo $j$ Ahora el timestamp asociado a cada mensaje es un vector: ts(m) = VC\nPara comparar dos relojes vectoriales, VCa \u0026lt; VCb si y solo si:\nVCa[k] \u0026lt;= VCb[k] para cualquier k existe al menos un k\u0026rsquo; tal que VCa[k\u0026rsquo;] \u0026lt; VCb[k\u0026rsquo;] Si se cumple VCa \u0026lt; VCb entonces un evento precede causalmente a otro.\nImplementación: similar al reloj lógico\nEjemplo de uso: multicast totalmente ordenado.\nExclusión mutua # Coordinar el acceso exclusivo a un recurso.\nEstrategias:\nMediante permisos: acuerdo entre los procesos. Utilizando un token: quien tiene el token, accede al recurso. Centralizado # Uso de un coordinador que otorga el acceso al recurso Facil de implementar, sencillo de mantener. Posibles problemas para escalar. Descentralizado # Recurso con N replicas, cada una con un coordinador asignado. Acceder al recurso requiere votos positivos de al menos m \u0026gt; N/2 coordinadores. Distribuido # Uso de multicast totalmente ordenado para coordinar el acceso. Requiere poder contactar a todos los procesos interesados en el mismo recurso. Token ring # Procesos organizados en un anillo lógico. Token circula por el anillo. Quien tiene el token puede acceder al recurso. Comparación: # Centralizado: Requiere 3 mensajes para acceder/liberar el recurso (petición, recepción del ok, liberación). Distribuido: Si existen N nodos, debo envíar mensajes a cada uno y esperar confirmación de ok: 2(N-1) mensajes. Token-ring: El token puede recorrer indefinidamente el anillo hasta ser retenido para el acceso al recurso. En el peor caso un nodo debe esperar N-1 mensajes hasta que le llegue el token (suponiendo un anillo de N nodos) Ejemplo: exclusion mutua con Zookeeper # En la práctica muchos sistemas distribuidos utilizan un coordinador centralizado\nZookeeper ofrece servicios para:\nexclusion mutua elección de lider monitoreo etc Diseñado para ofrecer confiabilidad, tolerancia a fallas y escalabilidad\nAunque es logicamente un servicio centralizado, su implementación es un sistema distribuido Usar zookeeper o servicios similares: ¡no hay que reinventar la rueda! (sobre todo una rueda complicada)\nZookeeper 101:\nNo hay primitivas bloqueantes Las peticiones de un cliente siempre reciben una respuesta. Ofrece un espacio de nombres, similar a un sistema de archivos. Operaciones: crear y eliminar nodos leer y actualizar datos en nodos (las actualizaciones son completas, no parciales) verificar si existe un nodo en particular Tipos de nodos: persistentes: deben ser creados y eliminados explicitamente efímeros: son eliminados cuando la conexión del proceso que los creo se pierde Servicio de notificaciones Evita polling por parte de los clientes. Ejemplo: obtener acceso exclusivo\nUn proceso crea un nodo, por ejemplo con nombre /lock Si existe, la operación falla indicando que ya existe El proceso debe repetir la operación para obtenerlo En caso de crearlo, para liberar el acceso elimina el nodo /lock Problemas: ¿que sucede cuando un cliente crea /lock y desaparece? Proceso p2 puede solicitar notificaciones por /lock mientras /lock es eliminado Estas sutilezas y muchas más son manejadas por zookeeper. Algoritmos de elección # Muchos algoritmos distribuidos requieren que un nodo actue como coordinador.\nNo importa en general cual nodo en particular sea el coordinador\u0026hellip; pero alguien tiene que hacerlo.\nMediante un algoritmo de elección se escoje un nodo para que actue como coordinador.\nEn general se asume:\nCada proceso P cuenta con un identificador único id(P). Cada proceso conoce a todo el conjunto de procesos (aunque no cuales estan funcionando). El objetivo de estos algoritmos es que cuando finalice la elección todos los procesos hayan acordado el mismo lider.\nAlgoritmo del matón (bully) # Considerar N procesos, cada uno con un identificador k, con k entre 0 y n-1. Cuando un proceso k se da cuenta que el lider no responde: Envía un mensaje ELECTION a todos los nodos con identificador \u0026gt; k. Si ninguno responde, el nodo k asume el papel de líder. Si alguno responde con OK, toma el control del proceso de elección y k desiste. Eventualmente, sólo un proceso tomará el control, enviando el mensaje COORDINATOR. Si un proceso caído retoma su ejecución, inicia una elección. Como el proceso con mayor ID es el que gana, se lo conoce por el nombre de \u0026ldquo;bully algorithm\u0026rdquo;. Elección en un anillo # Suponer que cada nodo conoce su sucesor, y al siguiente a este, y al proximo, y así.\nCuando un nodo detecta que el coordinador no responde:\nEnvía un mensaje ELECTION a su sucesor (o al siguiente si este no responde), con su ID. El receptor reenvia el mensaje ELECTION, agregando su propio ID. Eventualmente, el mensaje retorna al emisor original. En ese momento, el mensaje circula nuevamente ahora con el tipo COORDINATOR. El mensaje contiene ahora: el nuevo coordinador (el ID mas alto) y que nodos estan activos en el anillo. ¿Importa que dos o más procesos inicien una elección?\nNo, únicamente habrá mayor recarga en la red. Elecciones en sistemas de gran escala # Muchos algoritmos de elección suponen un número pequeño de nodos.\nLas cosas se vuelven complicadas a medida que el número de nodos aumenta.\nUn ejemplo es una red blockchain.\nProof of work # Consiste en que los nodos compitan en base a su poder de cómputo\nPara esto, compiten para ver quien es el primero en resolver un problema complejo pero soluble.\nEl ganador es el nodo que primero difunde una solución.\nEl nodo ganador se convierte en el líder: es quien añade la transacción a la cadena de bloques.\nMultiples problemas:\nPrincipalmente, consumo de energía. ¿Cómo regular la complejidad del problema? Proof of stake # Elecciones en redes inalambricas # En una red inalambrica, la transmisión no es necesariamente confiable, ni la topología permanece estática.\nEl algoritmo presentado por Vasudevan escoge el mejor líder.\nPara elegir un líder un nodo difunde un mensaje ELECTION a sus vecinos.\nSi un nodo vecino hubiera recibido ya un mensaje ELECTION, simplemente retorna un ACK.\nCaso contrario, si recibe un mensaje ELECTION por primera vez recuerda al nodo emisor y retrasmite el mensaje.\nEn cuanto todos los nodos vecinos responda a esta retransmisión de ELECTION, el nodo responde al emisor original.\nCoordinación basada en rumores # Se puede utilizar rumores para recolectar información.\nConsensuar un mismo valor:\nCada nodo $P_i$ escoge un valor arbitrario $v_i$ Cuando dos nodos $P_i$ y $P_j$ intercambian datos: $v_i, v_j \\leftarrow (v_i + v_j)/2$ Eventualmente todos los nodos tendran el mismo valor (media de los valores iniciales) Estimar el número de nodos:\nEl nodo $P_1$ escoge $v_1=1$, el resto de los nodos $v_i=0$ Si hay $N$ nodos, eventualmente todos tendran $v_i=1/N$ Se puede estimar el tamaño de la red como $1/v_i$ Seleccionar un nodo al azar:\nCada nodo $P_i$ seleccionar un valor $m_i$ al azar y setea $v_i=m_i$ Al intercambiar datos $P_i$ y $P_j$ realizan $v_i, v_j \\leftarrow max{v_i, v_j}$ Si luego $m_i \u0026lt; v_i$, entonces el nodo $P_i$ pierde la competencia. Eventualmente un único nodo será el ganador. Aplicacion\nUn nodo al azar inicia el proceso de estimación de números de nodos. Si el numero de nodos es estable, se puede designar un nodo fijo para que realice el conteo. Caso contrario, se pueden utilizar epocas o bien un nodo al azar cada tanto realiza un conteo. Peer-sampling # ¿Cómo elegir un nodo al azar cuando no se conoce la totalidad de los nodos en el sistema?\nUn nodo podría tener toda la información, pero no es una solución escalable.\nUna solución es el uso de vistas parciales:\nCada nodo mantiene una lista de c nodos vecinos. Los nodos intercambian parte de sus listas parciales con otros nodos (en su vista parcial). Cada nodo actualiza su vista parcial, pero siempre manteniendo c nodos en la misma. Si esto se repite regularmente, escoger un nodo al azar de la vista parcial es estadisticamente indistinguible de hacerlo de la totalidad de los nodos.\nConstruccion de redes superpuestas # Es posible utilizar las vistas parciales para generar topologías estructuradas.\nUn posible protocolo para lograrlo estaría dividido en dos capas:\nUna capa inferior que mantiene la vista parcial y opera sobre la red no-estructurada. Una capa superior que genera una topología estructurada en base a la vista parcial. Rumores seguros # La velocidad de propagación de datos puede generar problemas de seguridad/confiabilidad.\nPor ejemplo, $c$ nodos pueden cooperar maliciosamente para cooptar la red:\nAl intercambiar las vistas parciales, estos nodos envian $c/2$ entradas que sólo referencian a alguno de estos $c$ nodos. Gradualmente, las vistas parciales de todos los nodos solo contienen referencias a un conjunto de estos $c$ nodos. Se busca tratar de detectar y prevenir comportamiento malicioso\nLos nodos maliciosos pueden ser detectados por el elevado número de referencias desde otros nodos (indegree)\nSin embargo, al detectarlos ya puede ser demasiado tarde\nUna manera de mitigar es requerir que los nodos generen estadísticas:\nAl intercambiar vistas parciales se puede realizar también estadísticas Es importante que no se sepa cuando se utilizan para actualizar la vista parcial o para generar estadísticas Un nodo malicioso no puede devolver siempre enlaces a otros nodos maliciosos, sería rápidamente descubierto No queda otra que, de vez en cuando, \u0026ldquo;jugar con las reglas\u0026rdquo; de los nodos benignos "},{"id":26,"href":"/posts/04-comunicacion-multicast/","title":"Multicast","section":"Posts","content":"¿Cómo enviar datos a múltiples receptores?\nExisten numerosas soluciones a nivel de protocoles de red y de transporte. Su principal desventaja es el costo en armar las rutas de difusión de datos.\nA nivel de aplicación, las redes p2p estructuradas facilitan la creación de estas rutas de difusión. Veremos técnicas de difusión a este nivel.\nBasada en árboles # La idea básica es que los nodos estan organizados en una red superpuesta, utilizada para difundir los datos.\nLas conexiónes lógicas pueden no ser óptima desde el punto de vista de los enlaces físicos.\nExisten básicamente dos alternativas para la topología:\nArbol: existe un único camino entre dos nodos cualesquiera de la red. Mesh: cada nodo tiene múltiples vecinos y por lo tanto requiere algún tipo de ruteo (existe más de un camino entre dos nodos cualesquiera) Principal diferencia: mesh ofrece mayor tolerancia a fallos que árbol.\nPrincipal desafío: ¿cómo construir la red superpuesta para la difusión?\nAdicional: ¿cómo construimos un árbol de difusión eficiente?\nLa calidad del árbol para multidifusión se puede medir con tres métricas:\nLink stress: ¿cuántas veces debe un paquete atrevesar el mismo enlace? Link stretch: la razón entre el número de saltos en la red superpuesta y los enlaces físicos. Tree cost: métrica global, como reducir el costo agregado de los enlaces. Situación: un nuevo nodo quiere sumarse a la red superpuesta.\nSe contacta con un nodo bien conocido. ¿Cómo decidir que nodo será su nodo padre en el árbol? Para evitar sobrecargar nodos, en general se pone un límite k de nodos vecinos. Este límite dificulta establecer el árbol, ya que agregar un nodo puede requerir una reconfiguración. Inundación (flooding) # Para minimizar el número de nodos que reciben un mensaje del cual no son destinatarios, es mejor construir una red superpuesta con los nodos destino.\nEj: si en una topología de árbol un mensaje solo es para los nodos hoja.\nPosible solución: diferentes redes superpuestas para cada grupo multicast. Desventaja: un nodo puede pertencer a varias redes superpuestas, lo que incrementa el costo de administración.\nUna técnica sencilla de diseminar información a todos los nodos es la inundación:\nEnviar el mensaje a todos los nodos vecinos, excepto de quien lo recibió. Si se mantiene referencia de los mensajes enviados, se puede evitar duplicados. Problema: ineficiente (gran cantidad de mensajes). Sólo sería eficiente si la red superpuesta fuera un árbol.\nSe puede mejorar la situación utilizando inundación probabilistica:\nUn nodo reenviara el mensaje m a un nodo vecino con una probabilidad p. El número total de mensajes decrece de manera lineal con p. Problema: A menor valor de p, más chances que existan nodos que no reciban el mensaje. Se puede entonces tener en cuenta también el número de nodos vecinos al momento de decidir si reenviar el mensaje o no. ¿Y si la red superpuesta tiene una topología estructurada? Las cosas se hacen más fáciles.\nEjemplo: hipercubo. Reenviar mensajes a nodos con una dimension superior. Total de mensajes: 2^n - 1.\nOtro ejemplo: chord.\nEpidemico # Diseminar información siguiendo un comportamiento similar a los contagios de enfermedades. Como \u0026ldquo;infectar\u0026rdquo; rapidamente todos los nodos con un nuevo dato.\nIdea: difundir rápidamente información utilizando únicamente información local a cada nodo.\nVentaja: es una técnica escalable, requiere pocas sincronizaciones entre nodos.\nSuponemos que las actualizaciones se inician en un único nodo.\nTerminología:\nInfectado: nodo que tiene un dato que desea transmitir. Susceptible: nodo que no ha visto aún este nuevo dato. Removido: nodo que no reenvia datos. Modelos de propagación:\nAntientropia: Un nodo P eligue al azar un nodo vecino Q para intercambiar datos.\nP puede sólo envíar datos a Q (push) P puede sólo requerir datos de Q (pull) P y Q intercambian datos (push-pull) Sólo utilizar pull no es eficiente si existen muchos nodos infectados: la probabilidad de escoger un nodo susceptible es baja. Usar push es conveniente cuando el número de nodos infectados es alto. Por lo tanto, la mejor estrategia es push-pull.\nRonda: intervalo de tiempo en el cual cada nodo intercambio datos con un nodo vecino al azar.\n¿Cuántas rondas se necesitan para difundir a todos los nodos una actualización? Orden: O(log(N))\nRumores # Variante de epidémico: si el nodo P contacta un nodo Q al azar para comunicar el dato x. Si Q ya conoce el dato, P dejará de transmitir el dato (con una probabilidad p).\nVentajas: difunde muy rapidamente las actualizaciones. Desventaja: probabilidad de que no todos los nodos sean contactados.\nIncluso con valores bajos de p existe la posibilidad de que algunos nodos no sean actualizados. Para valores altos de p se deben tomar acciones adicionales en caso de que se requiera que la mayoría o todos de los nodos sean actualizados.\nDirigido # Una presunción que se hace es que un nodo P puede contactar cualquier nodo Q de la red. Esto raramente es así (no se cuenta con una lista completa de los nodos).\nEliminar datos # Los algoritmos epidemicos son excelentes para difundir una actualización.\nProblema: es muy complicado difundir una eliminación.\nSi un nodo elimina el datos x y posteriormente recibe una mensaje viejo de actualización, lo interpretará como un dato nuevo.\nSolución: realizar borrados lógicos, reenviando certificados de defunción.\nProblema: acumulación de certificados.\nSi se sabe que el tiempo de propagación de una actualización es n, se puede eliminar un certificado luego de n\u0026hellip; pero por las dudas, ciertos nodos específicos mantienen copias de estos certificados.\n"},{"id":27,"href":"/posts/04-comunicacion-mom/","title":"Middleware orientado a mensajes (MoM)","section":"Posts","content":"RPC o RMI no siempre son apropiados. Ej:\nel receptor no esta funcionando al mismo tiempo que el emisor. no se ajustan a la arquitectura cliente/servidor. Alternativa: envío de mensajes.\nUso de sockets # Socket: abstraccion sobre un puerto, donde se puede escribir o leer datos, usando un protocolo específico (ej: TCP o UDP).\nNo presenta el nivel de abstracción necesario. Cualquier funcionalidad adicional debe ser implementada por la aplicación.\nUso de patrones # La mayoría de las comunicaciones realizadas por las aplicaciones pueden ser categorizadas en unos pocos patrones:\nEj: request-reply, publish-subscribe, pipeline\nEj de implementación: ZeroMQ.\nMPI # Uso de paso de mensajes en computación de alto perfomance, por ejemplo clusters. TCP esta orientado a su uso sobre IP, por lo cual no es necesariamente efectivo en estas situaciones.\nEl estándar MPI se definio para lograr interoperabilidad entre soluciones de paso de mensajes para este tipo de escenarios.\nEj: no asume que un error en la red es recuperable.\nConsidera grupos de procesos, donde cada proceso tiene un identificador (grupoID, procesoID). Un proceso puede pertenecer a mas de un grupo.\nMas de 650 operaciones definidas.\nComunicación persistente # Sistemas de manejos de colas: ofrecen soporte para la comunicación asincrónica persistente.\nIdea básica: las aplicaciones se comunican enviando mensajes a buzones. Estos mensajes pueden a su vez ser reenviados a otros servidores de colas. En general cada aplicación tiene asociada una cola de mensajes.\nGarantía: en general se da la garantía que el mensaje será puesto en la cola de mensajes del receptor, pero no que este lo leerá.\nEmisor y receptor quedan así totalmente desacoplados en tiempo y espacio.\nEl contenido de los mensajes es arbitrario, aunque posiblemente limitado en tamaño. Solamente debe estar correctamente indicado el receptor.\nPrimitivas: PUT, GET, POLL, NOTIFY.\nArquitectura de un MoM # En general las colas de mensajes son responsabilidad de un administrador de colas de mensajes (queue manager).\nEn general el administrador de colas de mensajes es un proceso separado del cliente y/o el emisor.\nEl administrador tiene la responsabilidad de \u0026ldquo;rutear\u0026rdquo; los mensajes correctamente.\nEn general las direcciones de las colas de mensajes deben proveer transparencia de ubicación.\nUna cuestión a tener en cuenta es cómo informar a los distinos administradores de las direcciones existentes.\nEn sistemas complejos, no es realista que un administrador conozca a todo el resto: se debe rutear los mensajes con información incompleta (problema analogo a los routers en una red IP). Se utiliza una red superpuesta.\nBrokers # Un uso común de los sistemas de mensajes es integrar aplicaciones nuevas y existentes en un sistema coherente (¿suena?)\nLa integración requiere que las aplicaciones comprendan los mensajes que reciben del resto.\nEsto requiere que cada aplicacion entienda la sintaxis y la semántica de los protocolos utilizados por el resto.\nSoluciones?\nCada aplicación convierte los mensajes: impráctico, en un sistema con N aplicaciones, se requieren N^2 convertidores. Protocolo común: no es realista, dada la heterogeneidad de las aplicaciones. Información de sintaxis en cada mensaje: ejemplo, con esquemas XML. Falta entender la semántica. Entonces? No se puede esconder la situación, por lo tanto se debe ofrecer un mecanismo lo más simple posible para las conversiones.\nUn broker es una aplicación en un sistema de mensajería que se encarga de la conversión de mensajes.\nMucho más que un convertidor, puede actuar también como un gateway a nivel de aplicación:\nManejo de publicación/subscripción. Prioridades. Multicasting. Logging. Balanceo de carga. Etc. Para todo esto, un broker maneja una serie de reglas de transformación, ruteo, etc., que deben ser configuradas por un experto.\nEjemplo: AMQP # Advanced Message-Queuing Protocol (AMQP).\nAMQP ofrece:\nUn servicio de mensajería. Un protocolo de mensajes. Una interfaz de mensajería para las aplicaciones. Comunicación:\nUna aplicación establece una conexión con el administrador de colas de mensajes. Una conexión incorpora múltiples canales de una sola vía. Conexión -\u0026gt; mayor tiempo de vida, estable Canal -\u0026gt; dinámica, tiempo de vida breve Sesión: agrupamiento lógico de dos canales para comunicación full-duplex Manejo de mensajes:\nTipos de nodos: productor, consumidor, cola Los mensajes pueden ser persistentes (los nodos intermedios deben poder recuperarlo luego de un error) "},{"id":28,"href":"/posts/04-comunicacion-rpc/","title":"Comunicación","section":"Posts","content":"Fundamental en un sistema distribuido. Las primitivas de comunicación que ofrece el sistema operativo pueden no tener el nivel de abstracción necesario.\nFundaciones # El modelo OSI # Modelo de siete capas, no utilizado en la práctica, pero que es una referencia útil acerca de como esta estructurado lógicamente el stack de comunicación.\nMiddleware # Los servicios middleware para un sistema distribuido estarían logicamente ubicados en las capas de sesión y presentación del modelo OSI, aunque también pueden incorporar servicios en la capa de aplicación.\nTipos de comunicación # El middleware puede ser visto como un servicio adicional que media en la comunicación en una arquitectura cliente/servidor.\nLos tipos de comunicación se pueden categorizar en:\nPersistente:\nEl mensaje es almacenado por el middleware todo el tiempo que sea necesario para realizar la entrega. El emisor no necesita esperar a que se complete la recepción. El receptor no tiene por que estar ejecutando al ser enviado el mensaje. Transitoria:\nEl mensaje es almacenado únicamente el tiempo suficiente para el envío, sólo si emisor y receptor estan ejecutando. Cualquier error descarta el mensaje. Asincrónica:\nEl emisor continua con la ejecución luego de envíar el mensaje, quiza sin confirmación de envío ni recepción. Sincrónica:\nEl emisor se bloquea hasa que el mensaje sea aceptado (envío, recepción, procesamiento). RPC # Los desarrolladores estan familiarizados con el paradigma procedural. Si un procedimiento esta diseñado de manera que funcione aislado, no hay impedimento en principio que pueda ejecutar en otra maquina.\nIdea básica de RPC: Permitir invocar funciones remotas como si fueran locales.\nIdea sencilla pero de implementación compleja. Contribuye a la transparencia de distribución, especialmente a la transparencia de acceso. Problemas: falta de un espacio de direcciones común, diferencia en arquitecturas, caída de alguno de los procesos que se comunican, etc.\nEl proceso cliente invoca una función local que se denomina stub, que tiene la misma sintaxis que la función remota deseada, pero que se encarga de agrupar los parámetros en un mensaje y enviarlo al servidor, esperar la respuesta y desampaquetar los datos y retornar el resultado de la invocación.\nEn el servidor ocurre algo análogo: una funcion stub recibe la petición, desempaqueta los parámetros e invoca la función local en el servidor, y luego envía la respuesta a al cliente.\nPaso de parámetros # Aspecto dificultoso del esquema RPC:\n¿Cómo interpretar los párametros? ¿Cómo asegurar la misma representación de los datos? Existen diferencias en las arquitecturas, por ejemplo ordenamiento de los bytes o tamaño de palabras. Distintos lenguajes tiene diferentes tipos de datos. Solución: enviar datos en un formato independiente de la maquina.\nPor ejemplo, se utiliza big endian para ordenar los bytes en los mensajes en la red. Acuerdo en la codificación de tipos basicos y complejos. ¿Cómo pasamos punteros?\nProhibirlos (no es realista) Serializar toda la estructura de datos (por ejemplo el arreglo, lista, etc) Generalmente se puede utilizar un handle. Por ejemplo, nombre de archivo o url. Soporte # Dos alternativas:\nEspecificar detalladamente funciones y parametros, para generar stubs.\nIndicar como empaquetar el nombre de la función y sus parámetros. Representación de los tipos de datos. Decidir en el mecanismo de comunicación, por ejemplo mediante TCP/IP. La interface es especificada mediante un IDL (Interface Definition Language). Mediante un programa específico, la descripción mediante IDL es compilada en stubs. Incorporar la funcionalidad en el lenguaje de programación.\nFacilita el desarrollo de la aplicación. Ej: Java cuenta con RMI (Remote Method Invocation) Descubrimiento:\n¿Cómo puede el cliente saber qué servidor implementa la funcionalidad requerida? Solucion 1: el servidor puede ser bien conocido. Solucion 2: usar un servicio de directorio: El servidor registra en un directorio el servicio que ofrece y su dirección. El cliente contacta el directorio y consulta por un servicio en particular. El cliente se conecta al servidor que le indica el directorio. Variantes # RPC sincrónico: el emisor espera a que el receptor ejecute la función. RPC asincrónico: el emisor sólo espera hasta la confirmación de recepción por parte del emisor. RPC diferido: RPC asincrónico más un callback que se ejecuta al recibir la respuesta del receptor. Alternativamente al callback, el cliente puede realizar un polling. one-way RPC: el emisor genera el RPC pero no espera ni siquiera la confirmación de recepción. RPC Multicast: uso de one-way RPC para enviar múltiples peticiones, posiblemente con un callback. La aplicación puede no conocer que se realiza un multicast, lo oculta el stub. Es posible que tampoco lo sepa el stub, si se realiza mediante multicast en la capa de transporte. ¿Cómo procesar las respuestas? ¿La primera unicamente, todas? Depende de la aplicación. "},{"id":29,"href":"/posts/02-arquitectura/","title":"Arquitecturas","section":"Posts","content":"Es fundamental una correcta organización para administrar la complejidad de un sistema distribuido.\nPodemos diferenciar:\nLa organización de los componentes de software (arquitectura de software). Cómo están físicamente instanciados (arquitectura del sistema). Estilo arquitectonico # Organización lógica de los componentes de software del sistema:\nComponentes. Unidad modular con interfaces bien definidas (reemplazable). Cómo se conectan y comunican. Conector: el mecanismo que media la comunicación entre los componentes. Qué datos intercambian. Cómo están configurados. Según como se configuran componentes y conectores, tenemos una arquitectura de software.\nArquitecturas tipicamente utilizadas en sistemas distribuidos son:\nArquitecturas por capas. Orientadas a los servicios. Publish-subscribe. Arquitectura por capas # Los componentes se organizan en capas. Un componente en la capa N invoca generalmente los servicios de un componente en la capa N-1. Excepcionalmente, un componente puede invocar un servicio de una capa superior (N+1).\nEjemplo clásico: protocolos de comunicación de redes (TCP/IP, OSI, etc).\nMuchas aplicaciones se organizan en capas siguiendo el siguiente estilo:\nCapa de presentación o interfaz de usuario. Capa de procesamiento o de negocio. Capa de datos o persistencia. Desventaja:\nExiste una dependencia fuerte entre las distintas capas. Orientadas a los servicios # Organización más imprecisa, donde cada componente encapsula un servicio. El sistema se estructura como una composición de servicios.\nOrientado a objetos (invocación de objetos remotos).\nMicroservicios (cada componente representa un servicio separado e independiente).\nBasada en recursos (REST).\nObjetos:\nCada componente corresponde con objeto. Se comunican mediante algún mecanismo de invocación. Encapsular datos y procedimientos dentro del objeto, ofreciendo una interface. Los objetos pueden estar distribuidos. Microservicios:\nMas info: https://microservices.io/ \u0026ldquo;same crap, but distributed\u0026rdquo; the biggest issue with microservices is that they convert nice errors with a stack trace to network errors SOA:\nLa aplicación es una composición de servicios. Estos pueden pertenecer a diferentes organizaciones administrativas. Ej: procesador de pagos. Basados en recursos:\nEn lugar de servicios, se consideran recursos. Popular por su simplicidad. Publish-subscribe # Separación de procesamiento y coordinación (comunicación y cooperación).\nEl sistema es visto como un conjunto de componentes autónomos.\nLograr que los componentes no tengan dependencias explicitas.\nLos componentes describen los eventos que le son de interés.\nEvent-based\nReferencialmente desacoplados Temporalmente acoplados Space-based\nReferencial y temporalmente desacoplados. Comunicación mediante tuplas Procesos ingresan tuplas en un espacio compartido. Recuperación mediante búsqueda Ambos tipos se pueden combinar (generar evento cuando tuplas de interés son ingresadas al espacio de intercambio)\nEn este caso, hablamos de publicación - subscripción.\n- Se deben describir los eventos de interes. - Generalmente como (atributo, valor) o (atributo, rango) El middleware # Facilita el desarrollo de un sistema distribuido. Es una capa que administra recursos y ofrece servicios comunes.\nEl principal objetivo es ayudar en la transparencia de distribución.\nPor ejemplo, se puede encargar de:\nAcceso a recursos remotos. Ofrecer servicios para la comunicación entre componentes. Servicios de seguridad y administración. Recuperación de fallas. Coordinación. Etc. Algunos ejemplos concretos en sistemas distribuidos:\nServicio de Mensajería: RabbitMQ, ZeroMQ, Llamadas a procedimientos remotos: RPC, RMI, gRPC Objetos distribuidos: CORBA Streaming de datos y eventos: Apache Kafka Monitores de transacciones distribuidas El middleware se puede organizar de varias maneras. Por ejemplo:\nWrappers: por aplicación o centralizado. Interceptores: Permite ejecutar código adicional durante la ejecución de un servicio. Arquitectura del sistema # Ubicación e interaccion de los componentes software junto con el hardware.\nPensar en clientes que piden servicios a servidores ayuda con la complejidad de los sistemas distribuidos.\nArquitectura en capas # Cliente-servidor:\nModelo más sencillo Procesos divididos en dos grupos: clientes que invocan un servicio implementado en servidores La invocacion puede usar una conexión no confiable (quiza usando operaciones idempotentes) o confiable (TCP, cuando la red no es fiable). No siempre se puede definir de manera precisa la separación entre cliente y servidor. Puede variar. Existen múltiples alternativas de como distribuir tres capas lógicas en un modelo cliente-servidor.\nDos capas (two-tier) Multicapa: distribuir las capas en múltiples máquinas, por ejemplo una arquitectura de 3-capas. Distribución vertical: componentes lógicos separados en máquinas separadas.\nArquitecturas simétricas # Distribución horizontal: dividir cada componente lógico (servidor, cliente).\npeer to peer (p2p): las funcionalidades del sistema están presentes en todos los procesos que constituyen el sistema. Un nodo puede actuar tanto como cliente o como servidor. La arquitectura del sistema toma la forma de una red sobrepuesta, que puede ser estructurada o no.\np2p estructurados # Los nodos se organizan según alguna topología concreta: anillo, árbol, matriz, etc. En general cada nodo es responsable de mantener un cierto conjunto de datos, identificados mediante un identificador (generalmente, una función hash de los datos). Así, la red p2p es básicamente un tabla hash distribuida. La topología define como debe realizarse el ruteo de una consulta al nodo correspondiente.\nEjemplo: chord\np2p no estructurados # No existe una topología predefinida y cada nodo mantiene una lista ad-hoc de nodos vecinos, lo que da como resultado un grafo aleatorio. Al momento de unirse, un nodo contacta un nodo bien conocido para obtener una lista inicial de vecinos. La búsqueda de datos requiere técnicas como inundación o random walks.\nInundación: se pasa la búsqueda a todos los nodos vecinos. Un nodo ignora una búsqueda que ya recibió. Puede responder al nodo que originó la búsqueda o al que se la reenvió. La búsqueda tiene un TTL asociado. TTL igual a 1 para buscar entre nodos vecinos.\nAleatorio: se pregunto a un nodo vecino al azar. Si no tiene el dato, este repite el procedimiento. Menor trafico, mayor tiempo de búsqueda. Se puede paralelizar y también tiene un TTL asociado.\nPor política: mantener una lista de nodos que han respondido peticiones, etc.\np2p jerárquicos # Para aliviar problemas de escalabilidad, un p2p no-estructurado puede tener nodos especiales que mantengan un índice de items o datos, conocidos como \u0026ldquo;super pares\u0026rdquo;.\nNodo \u0026ldquo;weak\u0026rdquo; se conecta a la red a travez de un super-par. Puede ser siempre el mismo o no. Para mejorar confiabilidad, puede requerirse conectarse a n \u0026gt; 1 superpares. Los nodos superpares se organizan en una red p2p propia (de ahí la jerarquía) Problemas: ¿Cómo elegir que nodo superpar utilizar? ¿Cómo elegir cuales seran superpares? Elección de lider. Ejemplo: bittorrent, CDN.\nArquitecturas híbridas # En la práctica, un sistema complejo abarca múltiples arquitecturas.\nCloud computing # Permite el acceso a un conjunto de recursos virtualizados de fácil acceso. Cúantos de estos recursos son necesarios y cómo serán usados, es definido dinámicamente: por ejemplo, si un cliente requiere más poder de cómputo, simplemente puede pedir procesador virtuales adicionales. Básicamente, una nube se organiza en cuatro capas:\nHardware: la capa más baja, que los clientes generalmente no ven, organizada en data-centers. Infraestructura: una capa de virtualización sobre los recursos de hardware, para ofrecerlos a los clientes. Plataforma: provee una capa de abstracción para la ejecución de aplicaciones y/o administración de recursos como almacenamiento. Aplicación: aplicaciones que pueden ser adaptadas por los clientes, como suites ofimáticas. Estas capas son accesibles mediante una multitud de interfaces (web-services, APIs, etc).\nA su vez, da lugar a tres capas de servicios diferenciados:\nIasS (Infraestructure-as-a-service) PaaS (Platform-as-a-service) SaaS (Software-as-a-service) FaaS (Function-as-a-service): ejemplo AWS Lambda. El uso de un servicio en la nube tiene similitudes con la arquitectura cliente-servidor. Sin embargo, el servidor es totalmente opaco al cliente: no se sabe donde ejecuta, como esta implementado, etc.\nEdge computing # Como colocar los recursos en el \u0026ldquo;límite\u0026rdquo; de la red, entre los dispositivos y la nube. Generalmente, en los ISPs. Complementa cloud para reducir la latencia y el uso de ancho de banda. También permite aumentar la confiabilidad, y puede ser necesario para cumplir con políticas de privacidad y seguridad. Aumenta la complejidad de la administración de la aplicación.\nEj: Akamai, Netflix CDN, IoT, etc.\nArgumentos a favor de edge computing:\nLatencia y ancho de banda: aunque el ancho de banda se ha incrementado en los últimos años, contar con los recursos más cerca del usuario final permite mejores garantías acerca del ancho de banda negociado. En cambio, la latencia es un problemas más complicado y donde existe un límite físico. En este caso, la cercanía reduce la latencia.\nConfiabilidad: Existen ciertos casos donde se debe garantizar el funcionamiento aún ante falta de conectividad a la nube.\nSeguridad y privacidad: por razones politicas/regulatorias ciertos datos no pueden ser subidos a la nube.\nDesafios en orquestación:\nRecursos: para garantizar la disponibilidad de recursos, ¿como deben ser asignados o provisionados?\nUbicación: Dónde y cúando los recursos deben ser hechos disponibles.\nSelección: no necesariamente el nodo más cercano es el mejor.\nBlockchain apps # Una presunción en el diseño de estas aplicaciones es que ningún nodo es confiable. Las transacciones son registras por un gran número de nodos participantes. Entre todos los nodos se mantiene una sola cadena de transacciones validadas. Dado que cada bloque es inmutable, la estructura de datos es fácilmente replicable.\nLa diferencia fundamental entre múltiples implementaciones es cuales nodos se encargan de realizar las validaciones (esto es, agregar nuevos bloques a la cadena). Agregar un bloque es básicamente llegar a un consenso entre los distintos nodos con rol de validador. Los tipos de consenso pueden ser:\nCentralizado: posible, pero en contra del diseño del sistema.\nDistribuido: un grupo preseleccionado de nodos se encarga de este rol.\nLos nodos llegan a un consenso, tolerando así participantes maliciosos. Si hay n nodos validadores, se toleran hasta k \u0026lt;= (n - 1)/3 nodos maliciosos/defectuosos. El problema es que en general n es un numero pequeño. Descentralizada: todos los nodos participantes llegan a un consenso.\nMediante consenso todos los nodos escogen un nodo que llevara adelante la validación. Este validación puede ser premiada. No todos los nodos desearan ser elegibles, principalmente por costo de la validación. "},{"id":30,"href":"/posts/01-introduccion/","title":"Introducción a los Sistemas Distribuidos","section":"Posts","content":"the #1 rule of distribute computing: Don’t distribute your computing! At least if you can in any way avoid it\nYou\u0026rsquo;re not Google. Your company will never be Google\u0026hellip; Is there a reason we can\u0026rsquo;t just do this all in Postgres?\nAvances imporantes han ocurrido en las últimas décadas:\nDesarrollo de microprocesadores potentes, pequeños y ecónomicos. Avance de las tecnologías de comunicaciones. Miniaturización de los sistemas de cómputo (ES, IoT, SoCs, etc). En la actualidad es relativamente sencillo desarrollar un sistema compuesto de múltiples computadoras conectadas por una red. Al estar las computadoras físicamente separadas se habla de un sistema distribuido.\nDefinición # You know you have a distributed system when the crash of a computer you have never heard of stops you from getting any work done. \u0026ndash; Leslie Lamport\nUna colección de elementos computacionales autónomos que dan la apariencia a sus usuarios de ser un sistema coherente \u0026ndash; Tanenbaum y Van Steen.\nAunque no existe una definición que sea ampliamente aceptada por toda la disciplina, la distribución de los componentes en diferentes sistemas comunicados mediante una red es una característica común.\nCaracteristica 1: elementos independientes # Nodos independientes que colaboran para alcanzar un objetivo común. Los nodos son heterogéneos. La comunicación entre los nodos se realiza mediante paso de mensajes. No se existe un reloj global (dificulta la sincronización y coordinación). La concurrencia y el paralelismo es natural. Organizado como una red superpuesta, estructurada o no-estructurada (ej: sistemas p2p). Fallas parciales (independientes). Se tiene que resolver cuestiones de organización y membresía (grupos cerrados, abiertos). Caracteristica 2: sistema coherente # El sistema se comporta de acuerdo a las expectativas de sus usuarios. Transparencia de distribución: no importa como, cuando ni donde se conecte al sistema, el usuario debe tener el mismos servicio. Sin embargo, no es posible (ni deseable) ocultar todos los detalles de la distribución del sistema. Fundamental poder lidiar con fallas parciales. Distribuido vs Descentralizado # Conceptos relacionados\nSistema distribuido: componentes colaboran para realizar una tarea o proveer un servicio.\nSistema descentralizado: componentes con mayor autonomía sin único punto de control.\nUn sistema distribuido puede estar logicamente centralizado: DNS.\nUn sistema descentralizado no tiene una autoridad central: blockchain (ej. Bitcoin)\nOtra vision es la siguiente:\nSistema integrativo (conectar sistemas existentes formando así uno nuevo) Sistema expansivo (agregar nodos a un sistema existente). Luego:\nSistema descentralizado: visión integrativa, los recursos se encuentran necesariamente dispersos. Sistema distribuido: visión expansiva, los recursos se encuentran suficientemente dispersos. Complejidad # Los sistemas distribuidos son inherentemente complejos. Los sistemas centralizados son más sencillos. La distribución no es un fin en sí mismo: considerar soluciones lo más simples posibles. Middleware # Los componentes y funciones comunmente usados en un sistema distribuido se agrupan en un middleware, una capa de software entre el sistema operativo y las aplicaciones que intenta abstraer los detalles escabrosos y ofrecer una interfaz más amigable.\nPor ejemplo, un middleware ofrece:\nComunicación (RPC, RMI, paso de mensajes, etc.) Manejo de transacciones. Composición de servicios. Confiabilidad. Objetivos de diseño # Sólo por que sea posible no quiere decir que diseñar un sistema distribuido sea siempre una buena idea.\nUn sistema distribuido debe poder satisfacer alguno de los siguientes objetivos, para que su implementación valga la pena:\nPermitir que los recursos sean más fácilmente accesibles. Ocultar en lo posible que los recursos están desperdigados (transparencia de distribución). Debe ser abierto. Debe poder ser escalable. Compartir recursos # Por cuestiones económicas. Mejorar la colaboración. Ejemplo clásico: p2p Transparencia de distribución # Un sistema distribuido debe (en lo posible) ocultar que los procesos y recursos estan fisicamente distribuidos: esto se conoce como transparencia de distribución.\nDiversos tipos:\nAcceso: ocultar cómo se accede al recurso y la representación de datos. Ubicación: ocultar la ubicación de un recurso. Reubicación: ocultar el hecho de que el recurso pueda cambiar su ubicación mientras esta en uso. Migración: ocultar el hecho de que un recurso cambie su ubicación. Replicación: ocultar el hecho de que existan múltiples copias de un mismo recurso. Concurrencia: ocultar el hecho de que un recurso pueda ser accedido por múltiples usuarios. Falla: ocultar la falla y recuperación de un recurso u objeto. No siempre es deseable o posible alcanzar el máximo grado de transparencia.\nEs imposible ocultar las latencias de una red WAN u ocultar la falla de un nodo. Compromiso entre el nivel de transparencia y perfomance: Mantener las replicas consistentes incurre en un costo temporal que no se puede ocultar. Se puede argumentar que es mejor exponer la distribución al usuario, en lugar de ocultarla. Abierto # Poder interactuar con otros sistemas. Requiere interfaces bien definidas.\nUn sistema abierto es aquel que permite que sus componentes sean utilizados en otros sistemas. También generalmente un sistema abierto esta compuesto por componentes de otros sistemas. Beneficia la interoperabilidad, portabilidad, composibility y extensibilidad. Una característica importante para lograr este objetivo es la de separar política de mecanismo, evitando soluciones monolíticas.\nEscalabilidad # La escalabilidad abarca tres dimensiones:\nTamaño: la facilidad con que se pueden sumar usuarios o recursos.\nGeneralmente relacionado con limites en capacidad de cómputo, almacenamiento y ancho de banda. Pueden ser formalmente analizados mediante teoría de colas. Geografía: los recursos y usuarios pueden estar desperdigados pero las latencias no afectan seriamente al sistema.\nEl principal problema es la comunicación sincrónica sobre enlaces con alta latencia. Las WANs ofrecen menor confiabilidad que una LAN, menor capacidad de ancho de banda. ¿Multicast / Broadcast? Posible en LANs, no tan así en WANs. Administrativa: el sistema abarca distintas unidades organizacionales pero aún así es fácilmente administrable.\nCómo resolver conflictos de políticas acerca de uso, pago, administración, seguridad, etc. Ejemplos:\nTamaño: incrementar fácilmente el número de usuarios o procesos. Geografía: poder aumentar la distancia entre nodos. Administrativo: integrar recursos de otra organización. Soluciones:\nEscalar verticalmente: simplemente incrementar la capacidad del servicio (computo, almacenamiento o ancho de banda). Escalar horizontalmente: Ocultar latencias: comunicación asíncrona, fat-clients, etc. Distribuir el trabajo: dividir un componente y dispersarlo por el sistema. Ej: mover computo al cliente (Java Applet), descentralizar un servicio (DNS), descentralizar contenido (WWW), etc. Replicar: contar con una copia cercana, cache, etc. Problemas de consistencia. Consistencia estricta requiere sincronización global (costoso, reduce escalabilidad). Tipos de sistemas distribuidos # A grandes rasgos, podemos clasificar los sistemas distribuidos en sistemas distribuidos de cómputo, de información y pervasivos.\nCómputo: Cluster: conjunto de sistemas interconectados por una red de alta velocidad. Grid: nodos dispersos, heterogéneos, diferentes organizaciones. Cloud: software/infraestructura como servicio. Edge Información: Integración de sistemas de información ya existentes; ofrecer servicios como transacciones distribuidas. Pervasiva: Sistemas móviles, embebidos, IoT. No existe una topología estática, ni conexión permanente, etc. Tres tipos: ubicuos, móviles y redes de sensores (los límites entre la categorías son difusos). Falacias # Desarrollar un (buen) sistema distribuido es una tarea ardua. Las siguientes falsas presunciones durante el diseño del sistema, traen como consecuencia complejidad innecesaria y errores:\nLa red es confiable. La red es segura. La red es homogenea. La topología no cambia. La latencia es cero. El ancho de banda es infinito. El costo del transporte es cero. Sólo existe un administrador. Algunas más:\nUn sistema centralizado no escala: Un sistema físicamente centralizado quizá no, pero uno lógicamente centralizado sí (ejemplo DNS). Un sistema centralizado tiene un unico punto de falla: Si, pero un solo punto de falla es más fácil de administrar, más fácil de arreglar. Soluciones: en el caso de DNS, cada root server es a su vez un cluster. ¿Que vamos a estudiar? # Arquitectura: ¿Como organizar el sistema? Procesos: ¿Procesos, hilos? Comunicacion: ¿Como comunicar entre los nodos? Coordinación: ¿Cómo coordinar acciones? ¿Y cómo hacerlo de una manera independiente de la aplicación? Nombres: ¿Cómo identificar los diferentes recursos? Consistencia y replicación: Si se replica, ¿como se maneja la consistencia? Tolerancia a fallas: como mantener el sistema funcionando ante la falla de un componente. Seguridad: asegurar acceso autorizado a los recursos. "},{"id":31,"href":"/about/","title":"About","section":"","content":"Página de la materia Sistemas Distribuidos de la carrera Licenciatura en Informática de la Facultad de Ingeniería de la Universidad Nacional de la Patagonia San Juan Bosco, Sede Puerto Madryn.\n"}]